{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stk_actor.agent import UnifiedSACPolicy\n",
    "from stk_actor.wrappers import PreprocessObservationWrapper, StuckStopWrapper, SkipFirstNStepsWrapper\n",
    "from stable_baselines3 import PPO, A2C\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from pystk2_gymnasium import AgentSpec\n",
    "\n",
    "import torch\n",
    "\n",
    "path = 'stk_actor/trained_agents/'\n",
    "agent = 'normed_behavioral_cloning_num5'\n",
    "\n",
    "tracks = [\n",
    "    'abyss',\n",
    "    'black_forest',\n",
    "    'candela_city',\n",
    "    'cocoa_temple',\n",
    "    'cornfield_crossing',\n",
    "    'fortmagma',\n",
    "    'gran_paradiso_island',\n",
    "    'hacienda',\n",
    "    'lighthouse',\n",
    "    'mines',\n",
    "    'minigolf',\n",
    "    'olivermath',\n",
    "    'ravenbridge_mansion',\n",
    "    'sandtrack',\n",
    "    'scotland',\n",
    "    'snowmountain',\n",
    "    'snowtuxpeak',\n",
    "    'stk_enterprise',\n",
    "    'volcano_island',\n",
    "    'xr591',\n",
    "    'zengarden',\n",
    "\n",
    "# # #   ==================   #\n",
    "\n",
    "#     'fortmagma',\n",
    "#     'ravenbridge_mansion',\n",
    "#     'snowmountain',\n",
    "#     'cocoa_temple',\n",
    "#     'sandtrack',    \n",
    "#     'scotland', \n",
    "#     'stk_enterprise',\n",
    "#     'volcano_island', # 1104\n",
    "#     'xr591', # 864   \n",
    "]\n",
    "\n",
    "karts = [12]\n",
    "n_envs = len(tracks)*len(karts)\n",
    "\n",
    "print('making', n_envs, 'environments')\n",
    "vec_env = make_vec_env(\n",
    "    \"supertuxkart/flattened_multidiscrete-v0\",\n",
    "    # seed=12,\n",
    "    n_envs=n_envs, \n",
    "    wrapper_class=lambda x : (\n",
    "        SkipFirstNStepsWrapper(\n",
    "            StuckStopWrapper(\n",
    "                PreprocessObservationWrapper(x, ret_dict=False, norm=True, agent_name=agent),\n",
    "                n=92,\n",
    "            ), \n",
    "            n=20,\n",
    "        )\n",
    "    ), \n",
    "    env_kwargs={\n",
    "    'render_mode':None, 'agent':AgentSpec(use_ai=False, name=\"walid\"), #'track':'minigolf', \n",
    "    'laps':1,\n",
    "    'difficulty':2, \n",
    "    'num_kart':12, #'difficulty':0\n",
    "})\n",
    "\n",
    "ix = 0\n",
    "for num_kart in enumerate(karts):\n",
    "    for track in enumerate(tracks):\n",
    "        venv = vec_env.envs[ix]\n",
    "        venv.env.default_track = track\n",
    "        venv.env.num_kart = num_kart\n",
    "        print(ix, track, )\n",
    "        ix+=1\n",
    "\n",
    "net_arch=[1024,1024,1024]\n",
    "activation_fn=torch.nn.Tanh\n",
    "filename = path+f'{agent}/statedict'\n",
    "\n",
    "action_dims = [space.n for space in vec_env.action_space]\n",
    "unified_policy = UnifiedSACPolicy(\n",
    "    vec_env.observation_space, \n",
    "    action_dims, \n",
    "    net_arch=net_arch, \n",
    "    activation_fn=activation_fn\n",
    ")\n",
    "unified_policy.load_state_dict(torch.load(filename, map_location='cpu'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = [(\n",
    "    # 2048,\n",
    "    10*n_envs,\n",
    "    300_000,\n",
    ")]\n",
    "for n_steps, total_timesteps in steps:\n",
    "    # model = PPO(\n",
    "    #     \"MlpPolicy\", \n",
    "    #     vec_env, \n",
    "    #     verbose=1, \n",
    "    #     policy_kwargs = dict(net_arch=net_arch, activation_fn=activation_fn,),\n",
    "    #     device='cpu',\n",
    "    #     learning_rate=0.0001,\n",
    "    #     batch_size=128,\n",
    "    #     n_epochs=100,\n",
    "    #     n_steps=n_steps,\n",
    "    #     tensorboard_log=\"./outputs/\",\n",
    "    #     clip_range=0.2,\n",
    "    # )\n",
    "    model = A2C(\n",
    "        \"MlpPolicy\", \n",
    "        vec_env, \n",
    "        verbose=1, \n",
    "        policy_kwargs = dict(net_arch=net_arch, activation_fn=activation_fn,),\n",
    "        device='cpu',\n",
    "        learning_rate=0.001,\n",
    "        n_steps=n_steps,\n",
    "        tensorboard_log=\"./outputs/\",\n",
    "        use_rms_prop=False,\n",
    "        normalize_advantage=True,\n",
    "    )\n",
    "    print('DOING', n_steps, total_timesteps)\n",
    "    model.policy.load_state_dict(unified_policy.shared.state_dict())\n",
    "    model.learn(total_timesteps=total_timesteps, progress_bar=True, log_interval=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unified_policy.shared.load_state_dict(model.policy.state_dict())\n",
    "# torch.save(unified_policy.state_dict(), 'stk_actor/trained_agents/normed_a2c_num5_best/statedict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unified_policy.shared.load_state_dict(model.policy.state_dict())\n",
    "# torch.save(unified_policy.state_dict(), 'stk_actor/trained_agents/normed_ppo_num5_best/statedict')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bbrl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
