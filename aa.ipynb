{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "making 21 environments\n",
      "..:: Antarctica Rendering Engine 2.0 ::..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5b/wsyz4crx22bg_7tpsgbw3tzw0000gn/T/ipykernel_47394/1402294167.py:17: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.mean = torch.load('buffer_mean_2', map_location='cpu')\n",
      "/var/folders/5b/wsyz4crx22bg_7tpsgbw3tzw0000gn/T/ipykernel_47394/1402294167.py:18: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.std = torch.load('buffer_std_2', map_location='cpu')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 (0, 'abyss')\n",
      "1 (1, 'black_forest')\n",
      "2 (2, 'candela_city')\n",
      "3 (3, 'cocoa_temple')\n",
      "4 (4, 'cornfield_crossing')\n",
      "5 (5, 'fortmagma')\n",
      "6 (6, 'gran_paradiso_island')\n",
      "7 (7, 'hacienda')\n",
      "8 (8, 'lighthouse')\n",
      "9 (9, 'mines')\n",
      "10 (10, 'minigolf')\n",
      "11 (11, 'olivermath')\n",
      "12 (12, 'ravenbridge_mansion')\n",
      "13 (13, 'sandtrack')\n",
      "14 (14, 'scotland')\n",
      "15 (15, 'snowmountain')\n",
      "16 (16, 'snowtuxpeak')\n",
      "17 (17, 'stk_enterprise')\n",
      "18 (18, 'volcano_island')\n",
      "19 (19, 'xr591')\n",
      "20 (20, 'zengarden')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5b/wsyz4crx22bg_7tpsgbw3tzw0000gn/T/ipykernel_47394/1402294167.py:323: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  unified_policy.load_state_dict(torch.load(filename, map_location='cpu'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from stk_actor.wrappers import StuckStopWrapper\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class PreprocessObservationWrapper(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        \"\"\"\n",
    "        A Gym wrapper to preprocess mixed observation space (continuous + discrete)\n",
    "        into a flat tensor.\n",
    "        \n",
    "        Args:\n",
    "            env: The Gym environment to wrap.\n",
    "        \"\"\"\n",
    "        super().__init__(env)\n",
    "        self.observation_space = self._get_flat_observation_space(env.observation_space)\n",
    "        self.mean = torch.load('buffer_mean_2', map_location='cpu')\n",
    "        self.std = torch.load('buffer_std_2', map_location='cpu')\n",
    "\n",
    "    def _get_flat_observation_space(self, observation_space):\n",
    "        \"\"\"\n",
    "        Create a flat observation space based on the original observation space.\n",
    "        \n",
    "        Args:\n",
    "            observation_space: Original observation space with 'continuous' and 'discrete' components.\n",
    "        \n",
    "        Returns:\n",
    "            A flattened observation space.\n",
    "        \"\"\"\n",
    "        continuous_dim = observation_space['continuous'].shape[0]\n",
    "        discrete_dims = sum(space.n for space in observation_space['discrete'])\n",
    "        flat_dim = continuous_dim + discrete_dims\n",
    "        return gym.spaces.Box(low=-float('inf'), high=float('inf'), shape=(flat_dim,), dtype=float)\n",
    "\n",
    "    def observation(self, obs):\n",
    "        \"\"\"\n",
    "        Process the observation into a flat tensor.\n",
    "        \n",
    "        Args:\n",
    "            obs: The raw observation from the environment.\n",
    "        \n",
    "        Returns:\n",
    "            A preprocessed flat tensor.\n",
    "        \"\"\"\n",
    "        continuous_obs, discrete_obs = obs['continuous'], obs['discrete']\n",
    "        continuous_tensor = torch.FloatTensor(continuous_obs)\n",
    "        \n",
    "        discrete_tensors = [\n",
    "            F.one_hot(torch.tensor(x), num_classes=num_classes.n).float()\n",
    "            for x, num_classes in zip(discrete_obs, self.env.observation_space['discrete'])\n",
    "        ]\n",
    "        \n",
    "        flat_tensor = torch.cat([continuous_tensor] + discrete_tensors)\n",
    "        normed_flat_tensor = (flat_tensor - self.mean) / (self.std + 1e-8)\n",
    "        return normed_flat_tensor\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium import Wrapper\n",
    "\n",
    "class SkipFirstNStepsWrapper(Wrapper):\n",
    "    def __init__(self, env, n):\n",
    "        super().__init__(env)\n",
    "        self.n = n\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        # Reset the environment\n",
    "        obs, info = self.env.reset(**kwargs)\n",
    "        # Skip the first n steps\n",
    "        for _ in range(self.n):\n",
    "            obs, _, done, truncated, info = self.env.step(self.env.action_space.sample())\n",
    "            if done or truncated:\n",
    "                obs, info = self.env.reset(**kwargs)\n",
    "        return obs, info\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "from typing import Dict, List, Tuple, Union, Type\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "\n",
    "def get_device(device: Union[torch.device, str] = \"auto\") -> torch.device:\n",
    "    if device == \"auto\":\n",
    "        device = \"cuda\"\n",
    "    device = torch.device(device)\n",
    "    if device.type == torch.device(\"cuda\").type and not torch.cuda.is_available():\n",
    "        return torch.device(\"cpu\")\n",
    "    return device\n",
    "\n",
    "class BaseFeaturesExtractor(nn.Module):\n",
    "    def __init__(self, observation_space: gym.Space, features_dim: int = 0) -> None:\n",
    "        super().__init__()\n",
    "        assert features_dim > 0\n",
    "        self._observation_space = observation_space\n",
    "        self._features_dim = features_dim\n",
    "    @property\n",
    "    def features_dim(self) -> int:\n",
    "        return self._features_dim\n",
    "\n",
    "def get_flattened_obs_dim(observation_space: spaces.Space) -> int:\n",
    "    if isinstance(observation_space, spaces.MultiDiscrete):\n",
    "        return sum(observation_space.nvec)\n",
    "    else:\n",
    "        return spaces.utils.flatdim(observation_space)\n",
    "\n",
    "class FlattenExtractor(BaseFeaturesExtractor):\n",
    "    def __init__(self, observation_space: gym.Space) -> None:\n",
    "        super().__init__(observation_space, get_flattened_obs_dim(observation_space))\n",
    "        self.flatten = nn.Flatten()\n",
    "    def forward(self, observations: torch.Tensor) -> torch.Tensor:\n",
    "        return self.flatten(observations)\n",
    "    \n",
    "class MlpExtractor(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        feature_dim: int,\n",
    "        net_arch: Union[List[int], Dict[str, List[int]]],\n",
    "        activation_fn: Type[nn.Module],\n",
    "        device: Union[torch.device, str] = \"auto\",\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        # device = torch.get_device(device)\n",
    "        policy_net: List[nn.Module] = []\n",
    "        value_net: List[nn.Module] = []\n",
    "        last_layer_dim_pi = feature_dim\n",
    "        last_layer_dim_vf = feature_dim\n",
    "\n",
    "        if isinstance(net_arch, dict):\n",
    "            pi_layers_dims = net_arch.get(\"pi\", []) \n",
    "            vf_layers_dims = net_arch.get(\"vf\", []) \n",
    "        else:\n",
    "            pi_layers_dims = vf_layers_dims = net_arch\n",
    "        for curr_layer_dim in pi_layers_dims:\n",
    "            policy_net.append(nn.Linear(last_layer_dim_pi, curr_layer_dim))\n",
    "            policy_net.append(activation_fn())\n",
    "            last_layer_dim_pi = curr_layer_dim\n",
    "        for curr_layer_dim in vf_layers_dims:\n",
    "            value_net.append(nn.Linear(last_layer_dim_vf, curr_layer_dim))\n",
    "            value_net.append(activation_fn())\n",
    "            last_layer_dim_vf = curr_layer_dim\n",
    "\n",
    "        self.latent_dim_pi = last_layer_dim_pi\n",
    "        self.latent_dim_vf = last_layer_dim_vf\n",
    "        self.policy_net = nn.Sequential(*policy_net)#.to(device)\n",
    "        self.value_net = nn.Sequential(*value_net)#.to(device)\n",
    "\n",
    "    def forward(self, features: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        :return: latent_policy, latent_value of the specified network.\n",
    "            If all layers are shared, then ``latent_policy == latent_value``\n",
    "        \"\"\"\n",
    "        return self.forward_actor(features), self.forward_critic(features)\n",
    "\n",
    "    def forward_actor(self, features: torch.Tensor) -> torch.Tensor:\n",
    "        return self.policy_net(features)\n",
    "\n",
    "    def forward_critic(self, features: torch.Tensor) -> torch.Tensor:\n",
    "        return self.value_net(features)\n",
    "\n",
    "    \n",
    "class Policy(nn.Module):\n",
    "    def __init__(self, observation_space, action_dims, net_arch, activation_fn,):\n",
    "        super().__init__()\n",
    "        self.features_extractor = FlattenExtractor(observation_space)\n",
    "        self.pi_features_extractor = self.features_extractor\n",
    "        self.vf_features_extractor = self.features_extractor\n",
    "        self.mlp_extractor = MlpExtractor(\n",
    "            self.features_extractor.features_dim,\n",
    "            net_arch=net_arch,\n",
    "            activation_fn=activation_fn,\n",
    "        )\n",
    "        self.action_net = nn.Linear(net_arch[-1], sum(action_dims))\n",
    "        self.value_net = nn.Linear(net_arch[-1], 1)\n",
    "\n",
    "\n",
    "class UnifiedSACPolicy(nn.Module):\n",
    "    def __init__(self, observation_space, action_dims, net_arch, activation_fn):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.shared = Policy(\n",
    "            observation_space,\n",
    "            action_dims,\n",
    "            net_arch=net_arch,\n",
    "            activation_fn=activation_fn\n",
    "        )\n",
    "        self.action_dims = action_dims\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.shared.features_extractor(x)\n",
    "        x = self.shared.mlp_extractor.policy_net(x)\n",
    "        x = self.shared.action_net(x)\n",
    "        return x\n",
    "    \n",
    "    def sample(self, x, deterministic=False):\n",
    "        logits = self.forward(x)\n",
    "        \n",
    "        # Split logits for each action dimension\n",
    "        split_logits = torch.split(logits, self.action_dims, dim=-1)\n",
    "        \n",
    "        actions = []\n",
    "        log_probs = []\n",
    "        probs = []\n",
    "        \n",
    "        for logit in split_logits:\n",
    "            distribution = Categorical(logits=logit)\n",
    "            if deterministic:\n",
    "                action = torch.argmax(logit, dim=-1)\n",
    "            else:\n",
    "                action = distribution.sample()\n",
    "            \n",
    "            log_prob = distribution.log_prob(action)\n",
    "            prob = F.softmax(logit, dim=-1)\n",
    "            \n",
    "            actions.append(action)\n",
    "            log_probs.append(log_prob)\n",
    "            probs.append(prob)\n",
    "        \n",
    "        return (\n",
    "            torch.stack(actions),\n",
    "            torch.stack(log_probs),\n",
    "            probs\n",
    "        )\n",
    "    \n",
    "#policy = torch.load('policy_512_512_512_512_SiLU_3_statedict', map_location='cuda')\n",
    "\n",
    "\n",
    "from stable_baselines3 import PPO, A2C\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "import gymnasium as gym\n",
    "from pystk2_gymnasium import AgentSpec\n",
    "from bbrl.agents.gymnasium import ParallelGymAgent, make_env\n",
    "from functools import partial\n",
    "\n",
    "tracks = [\n",
    "    'abyss',\n",
    "    'black_forest',\n",
    "    'candela_city',\n",
    "    'cocoa_temple',\n",
    "    'cornfield_crossing',\n",
    "    'fortmagma',\n",
    "    'gran_paradiso_island',\n",
    "    'hacienda',\n",
    "    'lighthouse',\n",
    "    'mines',\n",
    "    'minigolf',\n",
    "    'olivermath',\n",
    "    'ravenbridge_mansion',\n",
    "    'sandtrack',\n",
    "    'scotland',\n",
    "    'snowmountain',\n",
    "    'snowtuxpeak',\n",
    "    'stk_enterprise',\n",
    "    'volcano_island',\n",
    "    'xr591',\n",
    "    'zengarden',\n",
    "\n",
    "# #   ==================   #\n",
    "\n",
    "#     'fortmagma',\n",
    "#     'ravenbridge_mansion',\n",
    "#     'snowmountain',\n",
    "#     'cocoa_temple',\n",
    "#     'sandtrack',    \n",
    "#     'scotland', \n",
    "#     'stk_enterprise',\n",
    "#     'volcano_island', # 1104\n",
    "#     'xr591', # 864   \n",
    "]\n",
    "\n",
    "# karts = [4,12]\n",
    "karts = [12]\n",
    "n_envs = len(tracks)*len(karts)\n",
    "\n",
    "print('making', n_envs, 'environments')\n",
    "vec_env = make_vec_env(\n",
    "    \"supertuxkart/flattened_multidiscrete-v0\",\n",
    "    # seed=12,\n",
    "    n_envs=n_envs, \n",
    "    wrapper_class=lambda x : (\n",
    "        SkipFirstNStepsWrapper(\n",
    "            StuckStopWrapper(\n",
    "                PreprocessObservationWrapper(x),\n",
    "                n=92,\n",
    "            ), \n",
    "            n=19,\n",
    "        )\n",
    "    ), \n",
    "    env_kwargs={\n",
    "    'render_mode':None, 'agent':AgentSpec(use_ai=False, name=\"walid\"), #'track':'minigolf', \n",
    "    'laps':1,\n",
    "    'difficulty':2, \n",
    "    'num_kart':12, #'difficulty':0\n",
    "})\n",
    "\n",
    "ix = 0\n",
    "for num_kart in enumerate(karts):\n",
    "    for track in enumerate(tracks):\n",
    "        venv = vec_env.envs[ix]\n",
    "        venv.env.default_track = track\n",
    "        venv.env.num_kart = num_kart\n",
    "        print(ix, track, )\n",
    "        ix+=1\n",
    "\n",
    "\n",
    "\n",
    "net_arch=[1024,1024,1024]\n",
    "activation_fn=torch.nn.Tanh\n",
    "filename = 'policy_normed_1024_1024_1024_Tanh_statedict_2'\n",
    "\n",
    "action_dims = [space.n for space in vec_env.action_space]\n",
    "unified_policy = UnifiedSACPolicy(\n",
    "    vec_env.observation_space, \n",
    "    action_dims, \n",
    "    net_arch=net_arch, \n",
    "    activation_fn=activation_fn\n",
    ")\n",
    "unified_policy.load_state_dict(torch.load(filename, map_location='cpu'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "..:: Antarctica Rendering Engine 2.0 ::..\n",
      "..:: Antarctica Rendering Engine 2.0 ::..\n",
      "..:: Antarctica Rendering Engine 2.0 ::..\n",
      "..:: Antarctica Rendering Engine 2.0 ::..\n",
      "..:: Antarctica Rendering Engine 2.0 ::..\n",
      "..:: Antarctica Rendering Engine 2.0 ::..\n",
      "..:: Antarctica Rendering Engine 2.0 ::..\n",
      "..:: Antarctica Rendering Engine 2.0 ::..\n",
      "..:: Antarctica Rendering Engine 2.0 ::..\n",
      "..:: Antarctica Rendering Engine 2.0 ::..\n",
      "..:: Antarctica Rendering Engine 2.0 ::..\n",
      "..:: Antarctica Rendering Engine 2.0 ::..\n",
      "..:: Antarctica Rendering Engine 2.0 ::..\n",
      "..:: Antarctica Rendering Engine 2.0 ::..\n",
      "..:: Antarctica Rendering Engine 2.0 ::..\n",
      "..:: Antarctica Rendering Engine 2.0 ::..\n",
      "..:: Antarctica Rendering Engine 2.0 ::..\n",
      "..:: Antarctica Rendering Engine 2.0 ::..\n",
      "..:: Antarctica Rendering Engine 2.0 ::..\n",
      "..:: Antarctica Rendering Engine 2.0 ::..\n",
      "DOING 2048 1000000\n",
      "Logging to ./outputs/A2C_17\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6d40c573b2648da9e0341327f4394a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 580      |\n",
      "|    ep_rew_mean     | 437      |\n",
      "| time/              |          |\n",
      "|    fps             | 64       |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 666      |\n",
      "|    total_timesteps | 43008    |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 605      |\n",
      "|    ep_rew_mean        | 454      |\n",
      "| time/                 |          |\n",
      "|    fps                | 55       |\n",
      "|    iterations         | 2        |\n",
      "|    time_elapsed       | 1550     |\n",
      "|    total_timesteps    | 86016    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.251   |\n",
      "|    explained_variance | 0.141    |\n",
      "|    learning_rate      | 0.0006   |\n",
      "|    n_updates          | 1        |\n",
      "|    policy_loss        | 1.61     |\n",
      "|    value_loss         | 2.24e+03 |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 607      |\n",
      "|    ep_rew_mean        | 496      |\n",
      "| time/                 |          |\n",
      "|    fps                | 55       |\n",
      "|    iterations         | 3        |\n",
      "|    time_elapsed       | 2337     |\n",
      "|    total_timesteps    | 129024   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.262   |\n",
      "|    explained_variance | -0.0531  |\n",
      "|    learning_rate      | 0.0006   |\n",
      "|    n_updates          | 2        |\n",
      "|    policy_loss        | 4.69     |\n",
      "|    value_loss         | 3.64e+03 |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 595      |\n",
      "|    ep_rew_mean        | 519      |\n",
      "| time/                 |          |\n",
      "|    fps                | 57       |\n",
      "|    iterations         | 4        |\n",
      "|    time_elapsed       | 3011     |\n",
      "|    total_timesteps    | 172032   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.254   |\n",
      "|    explained_variance | 0.105    |\n",
      "|    learning_rate      | 0.0006   |\n",
      "|    n_updates          | 3        |\n",
      "|    policy_loss        | 3.78     |\n",
      "|    value_loss         | 3.21e+03 |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 599      |\n",
      "|    ep_rew_mean        | 502      |\n",
      "| time/                 |          |\n",
      "|    fps                | 55       |\n",
      "|    iterations         | 5        |\n",
      "|    time_elapsed       | 3900     |\n",
      "|    total_timesteps    | 215040   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.261   |\n",
      "|    explained_variance | 0.143    |\n",
      "|    learning_rate      | 0.0006   |\n",
      "|    n_updates          | 4        |\n",
      "|    policy_loss        | -2.08    |\n",
      "|    value_loss         | 2.28e+03 |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 641      |\n",
      "|    ep_rew_mean        | 540      |\n",
      "| time/                 |          |\n",
      "|    fps                | 55       |\n",
      "|    iterations         | 6        |\n",
      "|    time_elapsed       | 4622     |\n",
      "|    total_timesteps    | 258048   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.267   |\n",
      "|    explained_variance | 0.213    |\n",
      "|    learning_rate      | 0.0006   |\n",
      "|    n_updates          | 5        |\n",
      "|    policy_loss        | 4.89     |\n",
      "|    value_loss         | 2.76e+03 |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 634      |\n",
      "|    ep_rew_mean        | 507      |\n",
      "| time/                 |          |\n",
      "|    fps                | 56       |\n",
      "|    iterations         | 7        |\n",
      "|    time_elapsed       | 5337     |\n",
      "|    total_timesteps    | 301056   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.258   |\n",
      "|    explained_variance | 0.224    |\n",
      "|    learning_rate      | 0.0006   |\n",
      "|    n_updates          | 6        |\n",
      "|    policy_loss        | 0.686    |\n",
      "|    value_loss         | 2e+03    |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 632      |\n",
      "|    ep_rew_mean        | 506      |\n",
      "| time/                 |          |\n",
      "|    fps                | 55       |\n",
      "|    iterations         | 8        |\n",
      "|    time_elapsed       | 6206     |\n",
      "|    total_timesteps    | 344064   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.242   |\n",
      "|    explained_variance | 0.246    |\n",
      "|    learning_rate      | 0.0006   |\n",
      "|    n_updates          | 7        |\n",
      "|    policy_loss        | 1.97     |\n",
      "|    value_loss         | 2.39e+03 |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 646      |\n",
      "|    ep_rew_mean        | 494      |\n",
      "| time/                 |          |\n",
      "|    fps                | 54       |\n",
      "|    iterations         | 9        |\n",
      "|    time_elapsed       | 7134     |\n",
      "|    total_timesteps    | 387072   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.253   |\n",
      "|    explained_variance | 0.215    |\n",
      "|    learning_rate      | 0.0006   |\n",
      "|    n_updates          | 8        |\n",
      "|    policy_loss        | -0.465   |\n",
      "|    value_loss         | 2.2e+03  |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 623      |\n",
      "|    ep_rew_mean        | 472      |\n",
      "| time/                 |          |\n",
      "|    fps                | 55       |\n",
      "|    iterations         | 10       |\n",
      "|    time_elapsed       | 7802     |\n",
      "|    total_timesteps    | 430080   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.262   |\n",
      "|    explained_variance | 0.172    |\n",
      "|    learning_rate      | 0.0006   |\n",
      "|    n_updates          | 9        |\n",
      "|    policy_loss        | 0.565    |\n",
      "|    value_loss         | 2.41e+03 |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 586      |\n",
      "|    ep_rew_mean        | 440      |\n",
      "| time/                 |          |\n",
      "|    fps                | 56       |\n",
      "|    iterations         | 11       |\n",
      "|    time_elapsed       | 8429     |\n",
      "|    total_timesteps    | 473088   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.263   |\n",
      "|    explained_variance | 0.278    |\n",
      "|    learning_rate      | 0.0006   |\n",
      "|    n_updates          | 10       |\n",
      "|    policy_loss        | -0.0697  |\n",
      "|    value_loss         | 1.88e+03 |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 583      |\n",
      "|    ep_rew_mean        | 449      |\n",
      "| time/                 |          |\n",
      "|    fps                | 55       |\n",
      "|    iterations         | 12       |\n",
      "|    time_elapsed       | 9264     |\n",
      "|    total_timesteps    | 516096   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.266   |\n",
      "|    explained_variance | 0.251    |\n",
      "|    learning_rate      | 0.0006   |\n",
      "|    n_updates          | 11       |\n",
      "|    policy_loss        | -0.76    |\n",
      "|    value_loss         | 2.11e+03 |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 632      |\n",
      "|    ep_rew_mean        | 494      |\n",
      "| time/                 |          |\n",
      "|    fps                | 53       |\n",
      "|    iterations         | 13       |\n",
      "|    time_elapsed       | 10365    |\n",
      "|    total_timesteps    | 559104   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.264   |\n",
      "|    explained_variance | 0.265    |\n",
      "|    learning_rate      | 0.0006   |\n",
      "|    n_updates          | 12       |\n",
      "|    policy_loss        | 0.027    |\n",
      "|    value_loss         | 1.56e+03 |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 635      |\n",
      "|    ep_rew_mean        | 549      |\n",
      "| time/                 |          |\n",
      "|    fps                | 53       |\n",
      "|    iterations         | 14       |\n",
      "|    time_elapsed       | 11175    |\n",
      "|    total_timesteps    | 602112   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.285   |\n",
      "|    explained_variance | 0.248    |\n",
      "|    learning_rate      | 0.0006   |\n",
      "|    n_updates          | 13       |\n",
      "|    policy_loss        | 3.38     |\n",
      "|    value_loss         | 2.94e+03 |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 593      |\n",
      "|    ep_rew_mean        | 462      |\n",
      "| time/                 |          |\n",
      "|    fps                | 53       |\n",
      "|    iterations         | 15       |\n",
      "|    time_elapsed       | 11989    |\n",
      "|    total_timesteps    | 645120   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.271   |\n",
      "|    explained_variance | 0.176    |\n",
      "|    learning_rate      | 0.0006   |\n",
      "|    n_updates          | 14       |\n",
      "|    policy_loss        | 1.85     |\n",
      "|    value_loss         | 2.8e+03  |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 618      |\n",
      "|    ep_rew_mean        | 435      |\n",
      "| time/                 |          |\n",
      "|    fps                | 53       |\n",
      "|    iterations         | 16       |\n",
      "|    time_elapsed       | 12857    |\n",
      "|    total_timesteps    | 688128   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.261   |\n",
      "|    explained_variance | 0.245    |\n",
      "|    learning_rate      | 0.0006   |\n",
      "|    n_updates          | 15       |\n",
      "|    policy_loss        | -0.9     |\n",
      "|    value_loss         | 2.2e+03  |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 609      |\n",
      "|    ep_rew_mean        | 437      |\n",
      "| time/                 |          |\n",
      "|    fps                | 53       |\n",
      "|    iterations         | 17       |\n",
      "|    time_elapsed       | 13554    |\n",
      "|    total_timesteps    | 731136   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.267   |\n",
      "|    explained_variance | 0.314    |\n",
      "|    learning_rate      | 0.0006   |\n",
      "|    n_updates          | 16       |\n",
      "|    policy_loss        | -0.63    |\n",
      "|    value_loss         | 1.38e+03 |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 611      |\n",
      "|    ep_rew_mean        | 417      |\n",
      "| time/                 |          |\n",
      "|    fps                | 53       |\n",
      "|    iterations         | 18       |\n",
      "|    time_elapsed       | 14384    |\n",
      "|    total_timesteps    | 774144   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.263   |\n",
      "|    explained_variance | 0.224    |\n",
      "|    learning_rate      | 0.0006   |\n",
      "|    n_updates          | 17       |\n",
      "|    policy_loss        | 1.54     |\n",
      "|    value_loss         | 2.08e+03 |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 611      |\n",
      "|    ep_rew_mean        | 488      |\n",
      "| time/                 |          |\n",
      "|    fps                | 53       |\n",
      "|    iterations         | 19       |\n",
      "|    time_elapsed       | 15244    |\n",
      "|    total_timesteps    | 817152   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.268   |\n",
      "|    explained_variance | 0.244    |\n",
      "|    learning_rate      | 0.0006   |\n",
      "|    n_updates          | 18       |\n",
      "|    policy_loss        | -0.177   |\n",
      "|    value_loss         | 1.96e+03 |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 626      |\n",
      "|    ep_rew_mean        | 518      |\n",
      "| time/                 |          |\n",
      "|    fps                | 53       |\n",
      "|    iterations         | 20       |\n",
      "|    time_elapsed       | 16011    |\n",
      "|    total_timesteps    | 860160   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.255   |\n",
      "|    explained_variance | 0.276    |\n",
      "|    learning_rate      | 0.0006   |\n",
      "|    n_updates          | 19       |\n",
      "|    policy_loss        | 2.83     |\n",
      "|    value_loss         | 2.43e+03 |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 582      |\n",
      "|    ep_rew_mean        | 429      |\n",
      "| time/                 |          |\n",
      "|    fps                | 54       |\n",
      "|    iterations         | 21       |\n",
      "|    time_elapsed       | 16714    |\n",
      "|    total_timesteps    | 903168   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.255   |\n",
      "|    explained_variance | 0.285    |\n",
      "|    learning_rate      | 0.0006   |\n",
      "|    n_updates          | 20       |\n",
      "|    policy_loss        | 1.55     |\n",
      "|    value_loss         | 2.51e+03 |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 578      |\n",
      "|    ep_rew_mean        | 464      |\n",
      "| time/                 |          |\n",
      "|    fps                | 53       |\n",
      "|    iterations         | 22       |\n",
      "|    time_elapsed       | 17614    |\n",
      "|    total_timesteps    | 946176   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.254   |\n",
      "|    explained_variance | 0.224    |\n",
      "|    learning_rate      | 0.0006   |\n",
      "|    n_updates          | 21       |\n",
      "|    policy_loss        | -0.45    |\n",
      "|    value_loss         | 2.19e+03 |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 630      |\n",
      "|    ep_rew_mean        | 518      |\n",
      "| time/                 |          |\n",
      "|    fps                | 53       |\n",
      "|    iterations         | 23       |\n",
      "|    time_elapsed       | 18446    |\n",
      "|    total_timesteps    | 989184   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.264   |\n",
      "|    explained_variance | 0.344    |\n",
      "|    learning_rate      | 0.0006   |\n",
      "|    n_updates          | 22       |\n",
      "|    policy_loss        | 1.84     |\n",
      "|    value_loss         | 2.07e+03 |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 639      |\n",
      "|    ep_rew_mean        | 522      |\n",
      "| time/                 |          |\n",
      "|    fps                | 53       |\n",
      "|    iterations         | 24       |\n",
      "|    time_elapsed       | 19411    |\n",
      "|    total_timesteps    | 1032192  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.247   |\n",
      "|    explained_variance | 0.328    |\n",
      "|    learning_rate      | 0.0006   |\n",
      "|    n_updates          | 23       |\n",
      "|    policy_loss        | -0.274   |\n",
      "|    value_loss         | 1.93e+03 |\n",
      "------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from stable_baselines3 import A2C\n",
    "\n",
    "\n",
    "steps = [(\n",
    "    1024*2,\n",
    "    # 128,\n",
    "    1_000_000,\n",
    ")]\n",
    "for n_steps, total_timesteps in steps:\n",
    "    model = A2C(\n",
    "        \"MlpPolicy\", \n",
    "        vec_env, \n",
    "        verbose=1, \n",
    "        policy_kwargs = dict(net_arch=net_arch, activation_fn=activation_fn,),\n",
    "        device='cpu',\n",
    "        learning_rate=0.0006,\n",
    "        n_steps=n_steps,\n",
    "        tensorboard_log=\"./outputs/\",\n",
    "        # clip_range=0.2,\n",
    "    )\n",
    "    print('DOING', n_steps, total_timesteps)\n",
    "    model.policy.load_state_dict(unified_policy.shared.state_dict())\n",
    "    model.policy.load_state_dict(\n",
    "        PPO.load(\n",
    "            \"ppti_ppo4_2048_batch128_clip01_ent0001\", \n",
    "            custom_objects={'policy_kwargs' :  dict(net_arch=net_arch, activation_fn=activation_fn), }\n",
    "        ).policy.state_dict(),\n",
    "    )\n",
    "    model.learn(total_timesteps=total_timesteps, progress_bar=True, log_interval=1)\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ppti_a2c_2048_batch128_clip01_ent0001'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save(f'ppti_a2c_{n_steps}_batch128_clip01_ent0001')\n",
    "f'ppti_a2c_{n_steps}_batch128_clip01_ent0001'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bbrl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
