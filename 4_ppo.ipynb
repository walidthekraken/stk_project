{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..:: Antarctica Rendering Engine 2.0 ::..\n",
      "0 abyss\n",
      "1 black_forest\n",
      "2 candela_city\n",
      "3 cocoa_temple\n",
      "4 cornfield_crossing\n",
      "5 fortmagma\n",
      "6 gran_paradiso_island\n",
      "7 hacienda\n",
      "8 lighthouse\n",
      "9 mines\n",
      "10 minigolf\n",
      "11 olivermath\n",
      "12 ravenbridge_mansion\n",
      "13 sandtrack\n",
      "14 scotland\n",
      "15 snowmountain\n",
      "16 snowtuxpeak\n",
      "17 stk_enterprise\n",
      "18 volcano_island\n",
      "19 xr591\n",
      "20 zengarden\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5b/wsyz4crx22bg_7tpsgbw3tzw0000gn/T/ipykernel_31684/3477717921.py:205: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  unified_policy.load_state_dict(torch.load(filename, map_location='cpu'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from stk_actor.wrappers import PreprocessObservationWrapper\n",
    "import torch.nn as nn\n",
    "from torch.distributions import Categorical\n",
    "from typing import Dict, List, Tuple, Union, Type\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "from gymnasium import spaces\n",
    "\n",
    "def get_device(device: Union[torch.device, str] = \"auto\") -> torch.device:\n",
    "    if device == \"auto\":\n",
    "        device = \"cuda\"\n",
    "    device = torch.device(device)\n",
    "    if device.type == torch.device(\"cuda\").type and not torch.cuda.is_available():\n",
    "        return torch.device(\"cpu\")\n",
    "    return device\n",
    "\n",
    "class BaseFeaturesExtractor(nn.Module):\n",
    "    def __init__(self, observation_space: gym.Space, features_dim: int = 0) -> None:\n",
    "        super().__init__()\n",
    "        assert features_dim > 0\n",
    "        self._observation_space = observation_space\n",
    "        self._features_dim = features_dim\n",
    "    @property\n",
    "    def features_dim(self) -> int:\n",
    "        return self._features_dim\n",
    "\n",
    "def get_flattened_obs_dim(observation_space: spaces.Space) -> int:\n",
    "    if isinstance(observation_space, spaces.MultiDiscrete):\n",
    "        return sum(observation_space.nvec)\n",
    "    else:\n",
    "        return spaces.utils.flatdim(observation_space)\n",
    "\n",
    "class FlattenExtractor(BaseFeaturesExtractor):\n",
    "    def __init__(self, observation_space: gym.Space) -> None:\n",
    "        super().__init__(observation_space, get_flattened_obs_dim(observation_space))\n",
    "        self.flatten = nn.Flatten()\n",
    "    def forward(self, observations: torch.Tensor) -> torch.Tensor:\n",
    "        return self.flatten(observations)\n",
    "    \n",
    "class MlpExtractor(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        feature_dim: int,\n",
    "        net_arch: Union[List[int], Dict[str, List[int]]],\n",
    "        activation_fn: Type[nn.Module],\n",
    "        device: Union[torch.device, str] = \"auto\",\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        # device = torch.get_device(device)\n",
    "        policy_net: List[nn.Module] = []\n",
    "        value_net: List[nn.Module] = []\n",
    "        last_layer_dim_pi = feature_dim\n",
    "        last_layer_dim_vf = feature_dim\n",
    "\n",
    "        if isinstance(net_arch, dict):\n",
    "            pi_layers_dims = net_arch.get(\"pi\", []) \n",
    "            vf_layers_dims = net_arch.get(\"vf\", []) \n",
    "        else:\n",
    "            pi_layers_dims = vf_layers_dims = net_arch\n",
    "        for curr_layer_dim in pi_layers_dims:\n",
    "            policy_net.append(nn.Linear(last_layer_dim_pi, curr_layer_dim))\n",
    "            policy_net.append(activation_fn())\n",
    "            last_layer_dim_pi = curr_layer_dim\n",
    "        for curr_layer_dim in vf_layers_dims:\n",
    "            value_net.append(nn.Linear(last_layer_dim_vf, curr_layer_dim))\n",
    "            value_net.append(activation_fn())\n",
    "            last_layer_dim_vf = curr_layer_dim\n",
    "\n",
    "        self.latent_dim_pi = last_layer_dim_pi\n",
    "        self.latent_dim_vf = last_layer_dim_vf\n",
    "        self.policy_net = nn.Sequential(*policy_net)#.to(device)\n",
    "        self.value_net = nn.Sequential(*value_net)#.to(device)\n",
    "\n",
    "    def forward(self, features: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        :return: latent_policy, latent_value of the specified network.\n",
    "            If all layers are shared, then ``latent_policy == latent_value``\n",
    "        \"\"\"\n",
    "        return self.forward_actor(features), self.forward_critic(features)\n",
    "\n",
    "    def forward_actor(self, features: torch.Tensor) -> torch.Tensor:\n",
    "        return self.policy_net(features)\n",
    "\n",
    "    def forward_critic(self, features: torch.Tensor) -> torch.Tensor:\n",
    "        return self.value_net(features)\n",
    "\n",
    "    \n",
    "class Policy(nn.Module):\n",
    "    def __init__(self, observation_space, action_dims, net_arch, activation_fn,):\n",
    "        super().__init__()\n",
    "        self.features_extractor = FlattenExtractor(observation_space)\n",
    "        self.pi_features_extractor = self.features_extractor\n",
    "        self.vf_features_extractor = self.features_extractor\n",
    "        self.mlp_extractor = MlpExtractor(\n",
    "            self.features_extractor.features_dim,\n",
    "            net_arch=net_arch,\n",
    "            activation_fn=activation_fn,\n",
    "        )\n",
    "        self.action_net = nn.Linear(net_arch[-1], sum(action_dims))\n",
    "        self.value_net = nn.Linear(net_arch[-1], 1)\n",
    "\n",
    "\n",
    "class UnifiedSACPolicy(nn.Module):\n",
    "    def __init__(self, observation_space, action_dims, net_arch, activation_fn):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.shared = Policy(\n",
    "            observation_space,\n",
    "            action_dims,\n",
    "            net_arch=net_arch,\n",
    "            activation_fn=activation_fn\n",
    "        )\n",
    "        self.action_dims = action_dims\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.shared.features_extractor(x)\n",
    "        x = self.shared.mlp_extractor.policy_net(x)\n",
    "        x = self.shared.action_net(x)\n",
    "        return x\n",
    "    \n",
    "    def sample(self, x, deterministic=False):\n",
    "        logits = self.forward(x)\n",
    "        \n",
    "        # Split logits for each action dimension\n",
    "        split_logits = torch.split(logits, self.action_dims, dim=-1)\n",
    "        \n",
    "        actions = []\n",
    "        log_probs = []\n",
    "        probs = []\n",
    "        \n",
    "        for logit in split_logits:\n",
    "            distribution = Categorical(logits=logit)\n",
    "            if deterministic:\n",
    "                action = torch.argmax(logit, dim=-1)\n",
    "            else:\n",
    "                action = distribution.sample()\n",
    "            \n",
    "            log_prob = distribution.log_prob(action)\n",
    "            prob = F.softmax(logit, dim=-1)\n",
    "            \n",
    "            actions.append(action)\n",
    "            log_probs.append(log_prob)\n",
    "            probs.append(prob)\n",
    "        \n",
    "        return (\n",
    "            torch.stack(actions),\n",
    "            torch.stack(log_probs),\n",
    "            probs\n",
    "        )\n",
    "    \n",
    "#policy = torch.load('policy_512_512_512_512_SiLU_3_statedict', map_location='cuda')\n",
    "\n",
    "\n",
    "from stable_baselines3 import PPO, A2C\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "import gymnasium as gym\n",
    "from pystk2_gymnasium import AgentSpec\n",
    "from bbrl.agents.gymnasium import ParallelGymAgent, make_env\n",
    "from functools import partial\n",
    "\n",
    "vec_env = make_vec_env(\"supertuxkart/flattened_multidiscrete-v0\", seed=0, n_envs=21, wrapper_class=lambda x : (PreprocessObservationWrapper(x, ret_dict=False)), env_kwargs={\n",
    "    'render_mode':None, 'agent':AgentSpec(use_ai=False, name=\"walid\"), 'track':'minigolf', 'laps':2#'difficulty':0, #'num_kart':2, 'difficulty':0\n",
    "})\n",
    "\n",
    "tracks = ['abyss',\n",
    " 'black_forest',\n",
    " 'candela_city',\n",
    " 'cocoa_temple',\n",
    " 'cornfield_crossing',\n",
    " 'fortmagma',\n",
    " 'gran_paradiso_island',\n",
    " 'hacienda',\n",
    " 'lighthouse',\n",
    " 'mines',\n",
    " 'minigolf',\n",
    " 'olivermath',\n",
    " 'ravenbridge_mansion',\n",
    " 'sandtrack',\n",
    " 'scotland',\n",
    " 'snowmountain',\n",
    " 'snowtuxpeak',\n",
    " 'stk_enterprise',\n",
    " 'volcano_island',\n",
    " 'xr591',\n",
    " 'zengarden']\n",
    "for i,venv in enumerate(vec_env.envs):\n",
    "    print(i, tracks[i%len(tracks)])\n",
    "    venv.env.default_track = tracks[i%len(tracks)]\n",
    "\n",
    "\n",
    "net_arch=[512,512,512,512]\n",
    "activation_fn=torch.nn.SiLU\n",
    "filename = 'policy_512_512_512_512_SiLU_3_statedict'\n",
    "\n",
    "action_dims = [space.n for space in vec_env.action_space]\n",
    "unified_policy = UnifiedSACPolicy(\n",
    "    vec_env.observation_space, \n",
    "    action_dims, \n",
    "    net_arch=net_arch, \n",
    "    activation_fn=activation_fn\n",
    ")\n",
    "unified_policy.load_state_dict(torch.load(filename, map_location='cpu'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/bbrl/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py:155: UserWarning: You have specified a mini-batch size of 128, but because the `RolloutBuffer` is of size `n_steps * n_envs = 31500`, after every 246 untruncated mini-batches, there will be a truncated mini-batch of size 12\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=1500 and n_envs=21)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..:: Antarctica Rendering Engine 2.0 ::..\n",
      "..:: Antarctica Rendering Engine 2.0 ::..\n",
      "..:: Antarctica Rendering Engine 2.0 ::..\n",
      "..:: Antarctica Rendering Engine 2.0 ::..\n",
      "..:: Antarctica Rendering Engine 2.0 ::..\n",
      "..:: Antarctica Rendering Engine 2.0 ::..\n",
      "..:: Antarctica Rendering Engine 2.0 ::..\n",
      "..:: Antarctica Rendering Engine 2.0 ::..\n",
      "..:: Antarctica Rendering Engine 2.0 ::..\n",
      "..:: Antarctica Rendering Engine 2.0 ::..\n",
      "..:: Antarctica Rendering Engine 2.0 ::..\n",
      "..:: Antarctica Rendering Engine 2.0 ::..\n",
      "..:: Antarctica Rendering Engine 2.0 ::..\n",
      "..:: Antarctica Rendering Engine 2.0 ::..\n",
      "..:: Antarctica Rendering Engine 2.0 ::..\n",
      "..:: Antarctica Rendering Engine 2.0 ::..\n",
      "..:: Antarctica Rendering Engine 2.0 ::..\n",
      "..:: Antarctica Rendering Engine 2.0 ::..\n",
      "..:: Antarctica Rendering Engine 2.0 ::..\n",
      "..:: Antarctica Rendering Engine 2.0 ::..\n",
      "DOING 1500 3000000\n",
      "Logging to ./outputs/PPO_4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2b2c34a13484f4898b5d9dfa91b3669",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 937      |\n",
      "|    ep_rew_mean     | 540      |\n",
      "| time/              |          |\n",
      "|    fps             | 88       |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 357      |\n",
      "|    total_timesteps | 31500    |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 870       |\n",
      "|    ep_rew_mean          | 561       |\n",
      "| time/                   |           |\n",
      "|    fps                  | 86        |\n",
      "|    iterations           | 2         |\n",
      "|    time_elapsed         | 730       |\n",
      "|    total_timesteps      | 63000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.4866613 |\n",
      "|    clip_fraction        | 0.352     |\n",
      "|    clip_range           | 0.01      |\n",
      "|    entropy_loss         | -0.122    |\n",
      "|    explained_variance   | 0.899     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 79.5      |\n",
      "|    n_updates            | 10        |\n",
      "|    policy_gradient_loss | 0.0456    |\n",
      "|    value_loss           | 114       |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 850       |\n",
      "|    ep_rew_mean          | 572       |\n",
      "| time/                   |           |\n",
      "|    fps                  | 85        |\n",
      "|    iterations           | 3         |\n",
      "|    time_elapsed         | 1101      |\n",
      "|    total_timesteps      | 94500     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.5720984 |\n",
      "|    clip_fraction        | 0.402     |\n",
      "|    clip_range           | 0.01      |\n",
      "|    entropy_loss         | -0.126    |\n",
      "|    explained_variance   | 0.885     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 30.9      |\n",
      "|    n_updates            | 20        |\n",
      "|    policy_gradient_loss | 0.0859    |\n",
      "|    value_loss           | 129       |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 954       |\n",
      "|    ep_rew_mean          | 566       |\n",
      "| time/                   |           |\n",
      "|    fps                  | 85        |\n",
      "|    iterations           | 4         |\n",
      "|    time_elapsed         | 1473      |\n",
      "|    total_timesteps      | 126000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.5220405 |\n",
      "|    clip_fraction        | 0.437     |\n",
      "|    clip_range           | 0.01      |\n",
      "|    entropy_loss         | -0.135    |\n",
      "|    explained_variance   | 0.904     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 5.04      |\n",
      "|    n_updates            | 30        |\n",
      "|    policy_gradient_loss | 0.0898    |\n",
      "|    value_loss           | 127       |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1.07e+03  |\n",
      "|    ep_rew_mean          | 530       |\n",
      "| time/                   |           |\n",
      "|    fps                  | 85        |\n",
      "|    iterations           | 5         |\n",
      "|    time_elapsed         | 1846      |\n",
      "|    total_timesteps      | 157500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6807845 |\n",
      "|    clip_fraction        | 0.229     |\n",
      "|    clip_range           | 0.01      |\n",
      "|    entropy_loss         | -0.077    |\n",
      "|    explained_variance   | 0.919     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 26.3      |\n",
      "|    n_updates            | 40        |\n",
      "|    policy_gradient_loss | 0.0444    |\n",
      "|    value_loss           | 60.9      |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1.2e+03   |\n",
      "|    ep_rew_mean          | 456       |\n",
      "| time/                   |           |\n",
      "|    fps                  | 85        |\n",
      "|    iterations           | 6         |\n",
      "|    time_elapsed         | 2220      |\n",
      "|    total_timesteps      | 189000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.3089201 |\n",
      "|    clip_fraction        | 0.274     |\n",
      "|    clip_range           | 0.01      |\n",
      "|    entropy_loss         | -0.0807   |\n",
      "|    explained_variance   | 0.917     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 3.57      |\n",
      "|    n_updates            | 50        |\n",
      "|    policy_gradient_loss | 0.0474    |\n",
      "|    value_loss           | 51.4      |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1.35e+03   |\n",
      "|    ep_rew_mean          | 369        |\n",
      "| time/                   |            |\n",
      "|    fps                  | 84         |\n",
      "|    iterations           | 7          |\n",
      "|    time_elapsed         | 2594       |\n",
      "|    total_timesteps      | 220500     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.33570236 |\n",
      "|    clip_fraction        | 0.413      |\n",
      "|    clip_range           | 0.01       |\n",
      "|    entropy_loss         | -0.168     |\n",
      "|    explained_variance   | 0.932      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 47.8       |\n",
      "|    n_updates            | 60         |\n",
      "|    policy_gradient_loss | 0.0375     |\n",
      "|    value_loss           | 38.8       |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 1.48e+03 |\n",
      "|    ep_rew_mean          | 312      |\n",
      "| time/                   |          |\n",
      "|    fps                  | 84       |\n",
      "|    iterations           | 8        |\n",
      "|    time_elapsed         | 2968     |\n",
      "|    total_timesteps      | 252000   |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 1.465121 |\n",
      "|    clip_fraction        | 0.413    |\n",
      "|    clip_range           | 0.01     |\n",
      "|    entropy_loss         | -0.151   |\n",
      "|    explained_variance   | 0.955    |\n",
      "|    learning_rate        | 0.0001   |\n",
      "|    loss                 | 12.1     |\n",
      "|    n_updates            | 70       |\n",
      "|    policy_gradient_loss | 0.0448   |\n",
      "|    value_loss           | 31.6     |\n",
      "--------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 1.5e+03  |\n",
      "|    ep_rew_mean          | 271      |\n",
      "| time/                   |          |\n",
      "|    fps                  | 84       |\n",
      "|    iterations           | 9        |\n",
      "|    time_elapsed         | 3340     |\n",
      "|    total_timesteps      | 283500   |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 5.326346 |\n",
      "|    clip_fraction        | 0.504    |\n",
      "|    clip_range           | 0.01     |\n",
      "|    entropy_loss         | -0.139   |\n",
      "|    explained_variance   | 0.958    |\n",
      "|    learning_rate        | 0.0001   |\n",
      "|    loss                 | 1.81     |\n",
      "|    n_updates            | 80       |\n",
      "|    policy_gradient_loss | 0.0509   |\n",
      "|    value_loss           | 35.4     |\n",
      "--------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1.5e+03    |\n",
      "|    ep_rew_mean          | 287        |\n",
      "| time/                   |            |\n",
      "|    fps                  | 84         |\n",
      "|    iterations           | 10         |\n",
      "|    time_elapsed         | 3714       |\n",
      "|    total_timesteps      | 315000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11814453 |\n",
      "|    clip_fraction        | 0.223      |\n",
      "|    clip_range           | 0.01       |\n",
      "|    entropy_loss         | -0.087     |\n",
      "|    explained_variance   | 0.956      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 3.81       |\n",
      "|    n_updates            | 90         |\n",
      "|    policy_gradient_loss | 0.0213     |\n",
      "|    value_loss           | 44.4       |\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "steps = [(\n",
    "    #16384,\n",
    "    1500,\n",
    "    3_000_000,\n",
    ")]\n",
    "for n_steps, total_timesteps in steps:\n",
    "    model = PPO(\n",
    "        \"MlpPolicy\", \n",
    "        vec_env, \n",
    "        verbose=1, \n",
    "        policy_kwargs = dict(net_arch=net_arch, activation_fn=activation_fn,),\n",
    "        device='cpu',\n",
    "        learning_rate=0.0001,\n",
    "        batch_size=128,\n",
    "        n_steps=n_steps,\n",
    "        tensorboard_log=\"./outputs/\",\n",
    "        ent_coef=0.001,\n",
    "        clip_range=0.001,\n",
    "    )\n",
    "    print('DOING', n_steps, total_timesteps)\n",
    "    model.policy.load_state_dict(unified_policy.shared.state_dict())\n",
    "    model.policy.load_state_dict(\n",
    "        PPO.load(\n",
    "            \"ppti_ppo_1500\", \n",
    "            custom_objects={'policy_kwargs' : dict(net_arch=net_arch, activation_fn=activation_fn), }\n",
    "        ).policy.state_dict()\n",
    "    )\n",
    "    model.learn(total_timesteps=total_timesteps, progress_bar=True)#callback=SummaryWriterCallback())\n",
    "    model.save(f'ppti_ppo_{n_steps}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(f'ppti_ppo_{n_steps}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bbrl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
