{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pystk2_gymnasium.stk_wrappers import ConstantSizedObservations, PolarObservations, DiscreteActionsWrapper\n",
    "from pystk2_gymnasium.wrappers import FlattenerWrapper\n",
    "from stk_actor.wrappers import PreprocessObservationWrapper\n",
    "\n",
    "import ipyparallel\n",
    "import torch\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def eval_agent(args_list):\n",
    "\n",
    "    from stk_actor.agent import UnifiedSACPolicy\n",
    "    from stk_actor.wrappers import PreprocessObservationWrapper\n",
    "    from pystk2_gymnasium.stk_wrappers import ConstantSizedObservations, PolarObservations, DiscreteActionsWrapper\n",
    "    from pystk2_gymnasium.wrappers import FlattenerWrapper\n",
    "    from pystk2_gymnasium import MonoAgentWrapperAdapter\n",
    "    from stk_actor.wrappers import PreprocessObservationWrapper\n",
    "    from pystk2_gymnasium import AgentSpec\n",
    "    from bbrl.agents import Agents\n",
    "    from bbrl.agents.gymnasium import ParallelGymAgent, make_env\n",
    "    from bbrl.workspace import Workspace\n",
    "    from functools import partial\n",
    "    from stk_actor.actors import Actor, ArgmaxActor\n",
    "\n",
    "    agents = []\n",
    "    agents_spec = []\n",
    "    player_names = []\n",
    "\n",
    "    def get_action(workspace: Workspace, t: int):\n",
    "        name = \"action\"\n",
    "\n",
    "        if name in workspace.variables:\n",
    "            # Action is a tensor\n",
    "            action = workspace.get(name, t)\n",
    "        else:\n",
    "            # Action is a dictionary\n",
    "            action = {}\n",
    "            prefix = f\"{name}/\"\n",
    "            len_prefix = len(prefix)\n",
    "            for varname in workspace.variables:\n",
    "                if not varname.startswith(prefix):\n",
    "                    continue\n",
    "                keys = varname[len_prefix:].split(\"/\")\n",
    "                current = action\n",
    "                for key in keys[:-1]:\n",
    "                    current = current.setdefault(key, {})\n",
    "                current[keys[-1]] = workspace.get(varname, t)\n",
    "\n",
    "        return action\n",
    "\n",
    "\n",
    "    def dict_slice(k: int, object):\n",
    "        if isinstance(object, dict):\n",
    "            return {key: dict_slice(k, value) for key, value in object.items()}\n",
    "        return object[k]\n",
    "\n",
    "    def is_integer(n):\n",
    "        try:\n",
    "            float(n)\n",
    "        except ValueError:\n",
    "            return False\n",
    "        else:\n",
    "            return float(n).is_integer()\n",
    "\n",
    "    def wrapper_func(state_items, state_karts, state_paths, norm, agent_name, env):\n",
    "        return PreprocessObservationWrapper(\n",
    "            FlattenerWrapper(\n",
    "                DiscreteActionsWrapper(\n",
    "                    PolarObservations(\n",
    "                        ConstantSizedObservations(\n",
    "                            env,\n",
    "                            state_items = state_items,\n",
    "                            state_karts = state_karts, \n",
    "                            state_paths = state_paths,\n",
    "                        )\n",
    "                    )\n",
    "                )\n",
    "            ), \n",
    "            ret_dict=True, \n",
    "            norm=norm,\n",
    "            agent_name=agent_name\n",
    "        )\n",
    "    \n",
    "    interactive=False\n",
    "\n",
    "    wrapper_factories = {}\n",
    "    for agent_ix, (track, agent_name, statedict_path, net_arch, activation_fn, mean, std, state_items, state_karts, state_paths) in enumerate(args_list):\n",
    "        wrapper_factories[str(agent_ix)]=partial(wrapper_func, state_items, state_karts, state_paths, (std is not None and mean is not None), agent_name)\n",
    "        agents_spec.append(AgentSpec(name=agent_name))\n",
    "        player_names.append(agent_name)\n",
    "        \n",
    "    n_agents = len(args_list)\n",
    "\n",
    "    env = make_env(\n",
    "        \"supertuxkart/multi-full-v0\",\n",
    "        render_mode=None,\n",
    "        agents=agents_spec,\n",
    "        num_kart=n_agents,\n",
    "        track=track,\n",
    "        wrappers=[\n",
    "            partial(\n",
    "                    MonoAgentWrapperAdapter,\n",
    "                    keep_original=interactive,\n",
    "                    wrapper_factories=wrapper_factories,\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    for agent_ix, (track, agent_name, statedict_path, net_arch, activation_fn, mean, std, state_items, state_karts, state_paths) in enumerate(args_list):\n",
    "\n",
    "        actor = Actor(\n",
    "            env.observation_space[str(agent_ix)], env.action_space[str(agent_ix)],\n",
    "            net_arch=net_arch, \n",
    "            activation_fn=activation_fn,\n",
    "            state_dict_path=statedict_path,\n",
    "        )\n",
    "        agent = Agents(actor, ArgmaxActor())\n",
    "        agents.append(agent)\n",
    "\n",
    "    for agent in agents:\n",
    "        agent.eval()\n",
    "\n",
    "    workspaces = [Workspace() for _ in range(n_agents)]\n",
    "    print(\"Starting a race\")\n",
    "\n",
    "    done = False\n",
    "    obs, _ = env.reset()\n",
    "    choice = \"\"\n",
    "\n",
    "    # List possible keys\n",
    "    keys = []\n",
    "    for key, item in obs.items():\n",
    "        for subkey in item.keys():\n",
    "            keys.append((key, subkey))\n",
    "    keys.sort()\n",
    "\n",
    "    t = 0\n",
    "    while not done:\n",
    "\n",
    "        actions = {}\n",
    "        for ix in range(n_agents):\n",
    "            key = str(ix)\n",
    "            obs_agent = ParallelGymAgent._format_frame(obs[key])\n",
    "            for var_key, var_value in obs_agent.items():\n",
    "                workspaces[ix].set(f\"env/{var_key}\", t, var_value)\n",
    "            agents[ix](workspaces[ix], t=t)\n",
    "            action = get_action(workspaces[ix], t=t)\n",
    "            if isinstance(action, dict):\n",
    "                action = dict_slice(0, action)\n",
    "            else:\n",
    "                action = action[0]\n",
    "\n",
    "            actions[key] = action\n",
    "\n",
    "        obs, reward, terminated, truncated, info = env.step(actions)\n",
    "        done = terminated or truncated\n",
    "        t += 1\n",
    "\n",
    "    records = []\n",
    "    rewards = info[\"reward\"]\n",
    "    for ix in range(n_agents):\n",
    "        key = str(ix)\n",
    "        print(  # noqa: T201\n",
    "            f\"{rewards[key]}\\t{info['infos'][key]['position']}\"\n",
    "            f\"\\t{ix}\\t{player_names[ix]}\"\n",
    "        )\n",
    "        records.append({\n",
    "            'agent_name': player_names[ix],\n",
    "            'reward' : rewards[str(ix)],\n",
    "            'position' : info['infos'][str(ix)]['position'],\n",
    "            'track' : track,\n",
    "        })\n",
    "\n",
    "    return records\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'stk_actor/trained_agents/'\n",
    "\n",
    "agents = [\n",
    "    [\n",
    "        'normed_behavioral_cloning_num5',\n",
    "        path+'normed_behavioral_cloning_num5/statedict',\n",
    "        path+'normed_behavioral_cloning_num5/buffer_mean',\n",
    "        path+'normed_behavioral_cloning_num5/buffer_std',\n",
    "        5, 5 ,5,\n",
    "    ],\n",
    "    [\n",
    "        'normed_behavioral_cloning_num10',\n",
    "        path+'normed_behavioral_cloning_num10/statedict',\n",
    "        path+'normed_behavioral_cloning_num10/buffer_mean',\n",
    "        path+'normed_behavioral_cloning_num10/buffer_std',\n",
    "        10,10,10,\n",
    "    ],\n",
    "    # [\n",
    "    #     'non_normed_behavioral_cloning_num5',\n",
    "    #     path+'non_normed_behavioral_cloning_num5/statedict',\n",
    "    #     None,\n",
    "    #     None,\n",
    "    #     5, 5 ,5,\n",
    "    # ],\n",
    "    # [\n",
    "    #     'non_normed_behavioral_cloning_num10',\n",
    "    #     path+'non_normed_behavioral_cloning_num10/statedict',\n",
    "    #     None,\n",
    "    #     None,\n",
    "    #     10,10,10,\n",
    "    # ],\n",
    "    [\n",
    "        'normed_a2c_num5_no_init',\n",
    "        path+'normed_a2c_num5_no_init/statedict',\n",
    "        path+'normed_a2c_num5_no_init/buffer_mean',\n",
    "        path+'normed_a2c_num5_no_init/buffer_std',\n",
    "        5,5,5,\n",
    "    ],\n",
    "    [\n",
    "        'normed_ppo_num5_no_init',\n",
    "        path+'normed_ppo_num5_no_init/statedict',\n",
    "        path+'normed_ppo_num5_no_init/buffer_mean',\n",
    "        path+'normed_ppo_num5_no_init/buffer_std',\n",
    "        5,5,5,\n",
    "    ],\n",
    "    [\n",
    "        'normed_a2c_num5_best',\n",
    "        path+'normed_a2c_num5_best/statedict',\n",
    "        path+'normed_a2c_num5_best/buffer_mean',\n",
    "        path+'normed_a2c_num5_best/buffer_std',\n",
    "        5,5,5,\n",
    "    ],\n",
    "    [\n",
    "        'normed_ppo_num5_best',\n",
    "        path+'normed_ppo_num5_best/statedict',\n",
    "        path+'normed_ppo_num5_best/buffer_mean',\n",
    "        path+'normed_ppo_num5_best/buffer_std',\n",
    "        5,5,5,\n",
    "    ],\n",
    "]\n",
    "\n",
    "net_arch=[1024,1024,1024]\n",
    "activation_fn=torch.nn.Tanh\n",
    "\n",
    "tracks = [\n",
    "    'black_forest','olivermath','minigolf','gran_paradiso_island', 'candela_city', 'mines', 'snowmountain', 'abyss', 'cornfield_crossing', 'hacienda','lighthouse', 'snowtuxpeak', 'zengarden', 'fortmagma','ravenbridge_mansion', 'cocoa_temple', 'sandtrack', 'scotland', 'stk_enterprise', 'volcano_island','xr591',       \n",
    "]\n",
    "\n",
    "args_list_list = []\n",
    "\n",
    "for track in tracks:\n",
    "\n",
    "    args_list = []\n",
    "\n",
    "    for agent_name, statedict_path, mean_path, std_path, state_items, state_karts, state_paths in agents:\n",
    "        \n",
    "        mean, std = None, None\n",
    "        if mean_path is not None:   \n",
    "            mean = torch.load(mean_path, map_location='cpu', weights_only=True)\n",
    "        if std_path is not None:\n",
    "            std = torch.load(std_path, map_location='cpu', weights_only=True)\n",
    "\n",
    "        args_list.append(\n",
    "            (\n",
    "                track, agent_name, statedict_path, net_arch, activation_fn, mean, std, state_items, state_karts, state_paths,\n",
    "            )\n",
    "        )\n",
    "    args_list_list.append(args_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallel_run_episodes(args,):\n",
    "    client = ipyparallel.Client()\n",
    "    dview = client[:]\n",
    "\n",
    "    dview.push({'eval_agent': eval_agent})\n",
    "    results = dview.map(eval_agent, args, )\n",
    "\n",
    "    print('running:', len(args))\n",
    "    \n",
    "    records = []\n",
    "    for rec in tqdm.tqdm(results, total=len(args)):\n",
    "        records.extend(rec)\n",
    "\n",
    "    client.close()\n",
    "    return records\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running: 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21/21 [13:34<00:00, 38.76s/it]  \n"
     ]
    }
   ],
   "source": [
    "records = parallel_run_episodes(args_list_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>agent_name</th>\n",
       "      <th>reward</th>\n",
       "      <th>position</th>\n",
       "      <th>track</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>normed_ppo_num5_best</td>\n",
       "      <td>18.302157</td>\n",
       "      <td>1</td>\n",
       "      <td>snowmountain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>normed_a2c_num5_best</td>\n",
       "      <td>18.301644</td>\n",
       "      <td>1</td>\n",
       "      <td>hacienda</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>normed_behavioral_cloning_num10</td>\n",
       "      <td>18.302364</td>\n",
       "      <td>1</td>\n",
       "      <td>cornfield_crossing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>normed_ppo_num5_best</td>\n",
       "      <td>18.302083</td>\n",
       "      <td>1</td>\n",
       "      <td>candela_city</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>normed_ppo_num5_best</td>\n",
       "      <td>18.301851</td>\n",
       "      <td>1</td>\n",
       "      <td>scotland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>normed_a2c_num5_no_init</td>\n",
       "      <td>-0.100123</td>\n",
       "      <td>6</td>\n",
       "      <td>hacienda</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>normed_behavioral_cloning_num10</td>\n",
       "      <td>0.013430</td>\n",
       "      <td>6</td>\n",
       "      <td>fortmagma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>normed_ppo_num5_no_init</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>6</td>\n",
       "      <td>zengarden</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>normed_a2c_num5_no_init</td>\n",
       "      <td>-0.100021</td>\n",
       "      <td>6</td>\n",
       "      <td>gran_paradiso_island</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>normed_a2c_num5_no_init</td>\n",
       "      <td>-0.099877</td>\n",
       "      <td>6</td>\n",
       "      <td>lighthouse</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>126 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          agent_name     reward  position  \\\n",
       "41              normed_ppo_num5_best  18.302157         1   \n",
       "58              normed_a2c_num5_best  18.301644         1   \n",
       "49   normed_behavioral_cloning_num10  18.302364         1   \n",
       "29              normed_ppo_num5_best  18.302083         1   \n",
       "107             normed_ppo_num5_best  18.301851         1   \n",
       "..                               ...        ...       ...   \n",
       "56           normed_a2c_num5_no_init  -0.100123         6   \n",
       "79   normed_behavioral_cloning_num10   0.013430         6   \n",
       "75           normed_ppo_num5_no_init  -0.100000         6   \n",
       "20           normed_a2c_num5_no_init  -0.100021         6   \n",
       "62           normed_a2c_num5_no_init  -0.099877         6   \n",
       "\n",
       "                    track  \n",
       "41           snowmountain  \n",
       "58               hacienda  \n",
       "49     cornfield_crossing  \n",
       "29           candela_city  \n",
       "107              scotland  \n",
       "..                    ...  \n",
       "56               hacienda  \n",
       "79              fortmagma  \n",
       "75              zengarden  \n",
       "20   gran_paradiso_island  \n",
       "62             lighthouse  \n",
       "\n",
       "[126 rows x 4 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame.from_records(records)\n",
    "df = df.sort_values('position')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>agent_name</th>\n",
       "      <th>position_mean</th>\n",
       "      <th>position_std</th>\n",
       "      <th>reward_mean</th>\n",
       "      <th>reward_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>normed_a2c_num5_best</td>\n",
       "      <td>2.095238</td>\n",
       "      <td>0.920909</td>\n",
       "      <td>14.535377</td>\n",
       "      <td>5.521040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>normed_a2c_num5_no_init</td>\n",
       "      <td>5.095238</td>\n",
       "      <td>0.971242</td>\n",
       "      <td>1.007674</td>\n",
       "      <td>3.151234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>normed_behavioral_cloning_num10</td>\n",
       "      <td>2.809524</td>\n",
       "      <td>1.592215</td>\n",
       "      <td>12.449337</td>\n",
       "      <td>6.763896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>normed_behavioral_cloning_num5</td>\n",
       "      <td>3.476190</td>\n",
       "      <td>1.051939</td>\n",
       "      <td>6.560209</td>\n",
       "      <td>7.292203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>normed_ppo_num5_best</td>\n",
       "      <td>2.190476</td>\n",
       "      <td>1.219643</td>\n",
       "      <td>14.498195</td>\n",
       "      <td>5.682283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>normed_ppo_num5_no_init</td>\n",
       "      <td>5.333333</td>\n",
       "      <td>0.712697</td>\n",
       "      <td>0.241969</td>\n",
       "      <td>0.384940</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        agent_name  position_mean  position_std  reward_mean  \\\n",
       "0             normed_a2c_num5_best       2.095238      0.920909    14.535377   \n",
       "1          normed_a2c_num5_no_init       5.095238      0.971242     1.007674   \n",
       "2  normed_behavioral_cloning_num10       2.809524      1.592215    12.449337   \n",
       "3   normed_behavioral_cloning_num5       3.476190      1.051939     6.560209   \n",
       "4             normed_ppo_num5_best       2.190476      1.219643    14.498195   \n",
       "5          normed_ppo_num5_no_init       5.333333      0.712697     0.241969   \n",
       "\n",
       "   reward_std  \n",
       "0    5.521040  \n",
       "1    3.151234  \n",
       "2    6.763896  \n",
       "3    7.292203  \n",
       "4    5.682283  \n",
       "5    0.384940  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def std(x):\n",
    "    return x.std(ddof=0)\n",
    "\n",
    "\n",
    "df_grouped = df.groupby('agent_name', as_index=False)[['position', 'reward']].agg({'position':['mean',std],'reward':['mean', std]})\n",
    "df_grouped.columns = ['agent_name'] + [f\"{col[0]}_{col[1]}\" for col in df_grouped.columns[1:]]\n",
    "\n",
    "df_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrrr}\n",
      "\\toprule\n",
      "agent_name & position_mean & position_std & reward_mean & reward_std \\\\\n",
      "\\midrule\n",
      "AC2+ObsNormalization+Size5+BehavioralInit & 2.095238 & 0.920909 & 14.535377 & 5.521040 \\\\\n",
      "PPO+ObsNormalization+Size5+BehavioralInit & 2.190476 & 1.219643 & 14.498195 & 5.682283 \\\\\n",
      "BehavioralCloning+ObsNormalization+Size10 & 2.809524 & 1.592215 & 12.449337 & 6.763896 \\\\\n",
      "BehavioralCloning+ObsNormalization+Size5 & 3.476190 & 1.051939 & 6.560209 & 7.292203 \\\\\n",
      "AC2+ObsNormalization+Size5+NoInit & 5.095238 & 0.971242 & 1.007674 & 3.151234 \\\\\n",
      "PPO+ObsNormalization+Size5+NoInit & 5.333333 & 0.712697 & 0.241969 & 0.384940 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "df_grouped.sort_values('position_mean').replace(\n",
    "    {\n",
    "        'normed_a2c_num5_best':'AC2+ObsNormalization+Size5+BehavioralInit',\n",
    "        'normed_a2c_num5_no_init':'AC2+ObsNormalization+Size5+NoInit',\n",
    "        'normed_behavioral_cloning_num10':'BehavioralCloning+ObsNormalization+Size10',\n",
    "        'normed_behavioral_cloning_num5':'BehavioralCloning+ObsNormalization+Size5',\n",
    "        'normed_ppo_num5_best':'PPO+ObsNormalization+Size5+BehavioralInit',\n",
    "        'normed_ppo_num5_no_init':'PPO+ObsNormalization+Size5+NoInit',\n",
    "    }\n",
    ").to_latex(index=False)\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bbrl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
