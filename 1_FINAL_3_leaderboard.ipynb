{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pystk2_gymnasium.stk_wrappers import ConstantSizedObservations, PolarObservations, DiscreteActionsWrapper\n",
    "from pystk2_gymnasium.wrappers import FlattenerWrapper\n",
    "from stk_actor.wrappers import PreprocessObservationWrapper\n",
    "\n",
    "import ipyparallel\n",
    "import torch\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def eval_agent(args_list):\n",
    "\n",
    "    from stk_actor.agent import UnifiedSACPolicy\n",
    "    from stk_actor.wrappers import PreprocessObservationWrapper\n",
    "    from pystk2_gymnasium.stk_wrappers import ConstantSizedObservations, PolarObservations, DiscreteActionsWrapper\n",
    "    from pystk2_gymnasium.wrappers import FlattenerWrapper\n",
    "    from pystk2_gymnasium import MonoAgentWrapperAdapter\n",
    "    from stk_actor.wrappers import PreprocessObservationWrapper\n",
    "    from pystk2_gymnasium import AgentSpec\n",
    "    from bbrl.agents import Agents\n",
    "    from bbrl.agents.gymnasium import ParallelGymAgent, make_env\n",
    "    from bbrl.workspace import Workspace\n",
    "    from functools import partial\n",
    "    from stk_actor.actors import Actor, ArgmaxActor\n",
    "\n",
    "    agents = []\n",
    "    agents_spec = []\n",
    "    player_names = []\n",
    "\n",
    "    def get_action(workspace: Workspace, t: int):\n",
    "        name = \"action\"\n",
    "\n",
    "        if name in workspace.variables:\n",
    "            # Action is a tensor\n",
    "            action = workspace.get(name, t)\n",
    "        else:\n",
    "            # Action is a dictionary\n",
    "            action = {}\n",
    "            prefix = f\"{name}/\"\n",
    "            len_prefix = len(prefix)\n",
    "            for varname in workspace.variables:\n",
    "                if not varname.startswith(prefix):\n",
    "                    continue\n",
    "                keys = varname[len_prefix:].split(\"/\")\n",
    "                current = action\n",
    "                for key in keys[:-1]:\n",
    "                    current = current.setdefault(key, {})\n",
    "                current[keys[-1]] = workspace.get(varname, t)\n",
    "\n",
    "        return action\n",
    "\n",
    "\n",
    "    def dict_slice(k: int, object):\n",
    "        if isinstance(object, dict):\n",
    "            return {key: dict_slice(k, value) for key, value in object.items()}\n",
    "        return object[k]\n",
    "\n",
    "    def is_integer(n):\n",
    "        try:\n",
    "            float(n)\n",
    "        except ValueError:\n",
    "            return False\n",
    "        else:\n",
    "            return float(n).is_integer()\n",
    "\n",
    "    def wrapper_func(state_items, state_karts, state_paths, norm, agent_name, env):\n",
    "        return PreprocessObservationWrapper(\n",
    "            FlattenerWrapper(\n",
    "                DiscreteActionsWrapper(\n",
    "                    PolarObservations(\n",
    "                        ConstantSizedObservations(\n",
    "                            env,\n",
    "                            state_items = state_items,\n",
    "                            state_karts = state_karts, \n",
    "                            state_paths = state_paths,\n",
    "                        )\n",
    "                    )\n",
    "                )\n",
    "            ), \n",
    "            ret_dict=True, \n",
    "            norm=norm,\n",
    "            agent_name=agent_name\n",
    "        )\n",
    "    \n",
    "    interactive=False\n",
    "\n",
    "    wrapper_factories = {}\n",
    "    for agent_ix, (track, agent_name, statedict_path, net_arch, activation_fn, mean, std, state_items, state_karts, state_paths) in enumerate(args_list):\n",
    "        wrapper_factories[str(agent_ix)]=partial(wrapper_func, state_items, state_karts, state_paths, (std is not None and mean is not None), agent_name)\n",
    "        agents_spec.append(AgentSpec(name=agent_name))\n",
    "        player_names.append(agent_name)\n",
    "        \n",
    "    n_agents = len(args_list)\n",
    "\n",
    "    env = make_env(\n",
    "        \"supertuxkart/multi-full-v0\",\n",
    "        render_mode=None,\n",
    "        agents=agents_spec,\n",
    "        num_kart=n_agents,\n",
    "        track=track,\n",
    "        wrappers=[\n",
    "            partial(\n",
    "                    MonoAgentWrapperAdapter,\n",
    "                    keep_original=interactive,\n",
    "                    wrapper_factories=wrapper_factories,\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    for agent_ix, (track, agent_name, statedict_path, net_arch, activation_fn, mean, std, state_items, state_karts, state_paths) in enumerate(args_list):\n",
    "\n",
    "        actor = Actor(\n",
    "            env.observation_space[str(agent_ix)], env.action_space[str(agent_ix)],\n",
    "            net_arch=net_arch, \n",
    "            activation_fn=activation_fn,\n",
    "            state_dict_path=statedict_path,\n",
    "        )\n",
    "        agent = Agents(actor, ArgmaxActor())\n",
    "        agents.append(agent)\n",
    "\n",
    "    for agent in agents:\n",
    "        agent.eval()\n",
    "\n",
    "    workspaces = [Workspace() for _ in range(n_agents)]\n",
    "    print(\"Starting a race\")\n",
    "\n",
    "    done = False\n",
    "    obs, _ = env.reset()\n",
    "    choice = \"\"\n",
    "\n",
    "    # List possible keys\n",
    "    keys = []\n",
    "    for key, item in obs.items():\n",
    "        for subkey in item.keys():\n",
    "            keys.append((key, subkey))\n",
    "    keys.sort()\n",
    "\n",
    "    t = 0\n",
    "    while not done:\n",
    "\n",
    "        actions = {}\n",
    "        for ix in range(n_agents):\n",
    "            key = str(ix)\n",
    "            obs_agent = ParallelGymAgent._format_frame(obs[key])\n",
    "            for var_key, var_value in obs_agent.items():\n",
    "                workspaces[ix].set(f\"env/{var_key}\", t, var_value)\n",
    "            agents[ix](workspaces[ix], t=t)\n",
    "            action = get_action(workspaces[ix], t=t)\n",
    "            if isinstance(action, dict):\n",
    "                action = dict_slice(0, action)\n",
    "            else:\n",
    "                action = action[0]\n",
    "\n",
    "            actions[key] = action\n",
    "\n",
    "        obs, reward, terminated, truncated, info = env.step(actions)\n",
    "        done = terminated or truncated\n",
    "        t += 1\n",
    "\n",
    "    records = []\n",
    "    rewards = info[\"reward\"]\n",
    "    for ix in range(n_agents):\n",
    "        key = str(ix)\n",
    "        print(  # noqa: T201\n",
    "            f\"{rewards[key]}\\t{info['infos'][key]['position']}\"\n",
    "            f\"\\t{ix}\\t{player_names[ix]}\"\n",
    "        )\n",
    "        records.append({\n",
    "            'agent_name': player_names[ix],\n",
    "            'reward' : rewards[str(ix)],\n",
    "            'position' : info['infos'][str(ix)]['position'],\n",
    "            'track' : track\n",
    "        })\n",
    "\n",
    "    return records\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'stk_actor/trained_agents/'\n",
    "\n",
    "agents = [\n",
    "    [\n",
    "        'normed_behavioral_cloning_num5',\n",
    "        path+'normed_behavioral_cloning_num5/statedict',\n",
    "        path+'normed_behavioral_cloning_num5/buffer_mean',\n",
    "        path+'normed_behavioral_cloning_num5/buffer_std',\n",
    "        5, 5 ,5,\n",
    "    ],\n",
    "    [\n",
    "        'normed_behavioral_cloning_num10',\n",
    "        path+'normed_behavioral_cloning_num10/statedict',\n",
    "        path+'normed_behavioral_cloning_num10/buffer_mean',\n",
    "        path+'normed_behavioral_cloning_num10/buffer_std',\n",
    "        10,10,10,\n",
    "    ],\n",
    "    # [\n",
    "    #     'non_normed_behavioral_cloning_num5',\n",
    "    #     path+'non_normed_behavioral_cloning_num5/statedict',\n",
    "    #     None,\n",
    "    #     None,\n",
    "    #     5, 5 ,5,\n",
    "    # ],\n",
    "    # [\n",
    "    #     'non_normed_behavioral_cloning_num10',\n",
    "    #     path+'non_normed_behavioral_cloning_num10/statedict',\n",
    "    #     None,\n",
    "    #     None,\n",
    "    #     10,10,10,\n",
    "    # ],\n",
    "    [\n",
    "        'normed_a2c_num5_no_init',\n",
    "        path+'normed_a2c_num5_no_init/statedict',\n",
    "        path+'normed_a2c_num5_no_init/buffer_mean',\n",
    "        path+'normed_a2c_num5_no_init/buffer_std',\n",
    "        5,5,5,\n",
    "    ],\n",
    "    [\n",
    "        'normed_ppo_num5_no_init',\n",
    "        path+'normed_ppo_num5_no_init/statedict',\n",
    "        path+'normed_ppo_num5_no_init/buffer_mean',\n",
    "        path+'normed_ppo_num5_no_init/buffer_std',\n",
    "        5,5,5,\n",
    "    ],\n",
    "    [\n",
    "        'normed_a2c_num5_best',\n",
    "        path+'normed_a2c_num5_best/statedict',\n",
    "        path+'normed_a2c_num5_best/buffer_mean',\n",
    "        path+'normed_a2c_num5_best/buffer_std',\n",
    "        5,5,5,\n",
    "    ],\n",
    "    [\n",
    "        'normed_ppo_num5_best',\n",
    "        path+'normed_ppo_num5_best/statedict',\n",
    "        path+'normed_ppo_num5_best/buffer_mean',\n",
    "        path+'normed_ppo_num5_best/buffer_std',\n",
    "        5,5,5,\n",
    "    ],\n",
    "]\n",
    "\n",
    "net_arch=[1024,1024,1024]\n",
    "activation_fn=torch.nn.Tanh\n",
    "\n",
    "tracks = [\n",
    "    'black_forest','olivermath','minigolf','gran_paradiso_island', 'candela_city', 'mines', 'snowmountain', 'abyss', 'cornfield_crossing', 'hacienda','lighthouse', 'snowtuxpeak', 'zengarden', 'fortmagma','ravenbridge_mansion', 'cocoa_temple', 'sandtrack', 'scotland', 'stk_enterprise', 'volcano_island','xr591',       \n",
    "]\n",
    "\n",
    "args_list_list = []\n",
    "\n",
    "for track in tracks:\n",
    "\n",
    "    args_list = []\n",
    "\n",
    "    for agent_name, statedict_path, mean_path, std_path, state_items, state_karts, state_paths in agents:\n",
    "        \n",
    "        mean, std = None, None\n",
    "        if mean_path is not None:   \n",
    "            mean = torch.load(mean_path, map_location='cpu', weights_only=True)\n",
    "        if std_path is not None:\n",
    "            std = torch.load(std_path, map_location='cpu', weights_only=True)\n",
    "\n",
    "        args_list.append(\n",
    "            (\n",
    "                track, agent_name, statedict_path, net_arch, activation_fn, mean, std, state_items, state_karts, state_paths,\n",
    "            )\n",
    "        )\n",
    "    args_list_list.append(args_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallel_run_episodes(args,):\n",
    "    client = ipyparallel.Client()\n",
    "    dview = client[:]\n",
    "\n",
    "    dview.push({'eval_agent': eval_agent})\n",
    "    results = dview.map(eval_agent, args, )\n",
    "\n",
    "    print('running:', len(args))\n",
    "    \n",
    "    records = []\n",
    "    for rec in tqdm.tqdm(results, total=len(args)):\n",
    "        records.extend(rec)\n",
    "\n",
    "    client.close()\n",
    "    return records\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running: 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/21 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "records = parallel_run_episodes(args_list_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>agent_name</th>\n",
       "      <th>reward</th>\n",
       "      <th>position</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>normed_ppo_num5_best</td>\n",
       "      <td>18.302193</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>normed_a2c_num5_best</td>\n",
       "      <td>16.635527</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>normed_behavioral_cloning_num10</td>\n",
       "      <td>14.968872</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>normed_behavioral_cloning_num5</td>\n",
       "      <td>13.302205</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>normed_ppo_num5_no_init</td>\n",
       "      <td>0.400058</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>normed_a2c_num5_no_init</td>\n",
       "      <td>-0.100317</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        agent_name     reward  position\n",
       "5             normed_ppo_num5_best  18.302193         1\n",
       "4             normed_a2c_num5_best  16.635527         2\n",
       "1  normed_behavioral_cloning_num10  14.968872         3\n",
       "0   normed_behavioral_cloning_num5  13.302205         4\n",
       "3          normed_ppo_num5_no_init   0.400058         5\n",
       "2          normed_a2c_num5_no_init  -0.100317         6"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame.from_records(records)\n",
    "df = df.sort_values('position')\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bbrl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
