{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..:: Antarctica Rendering Engine 2.0 ::..\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.ppo import MlpPolicy\n",
    "from imitation.algorithms import density as db\n",
    "from imitation.algorithms.adversarial.airl import AIRL\n",
    "from imitation.data import rollout\n",
    "from imitation.data.wrappers import RolloutInfoWrapper\n",
    "from imitation.policies.serialize import load_policy\n",
    "from imitation.rewards.reward_nets import BasicShapedRewardNet\n",
    "from imitation.util.networks import RunningNorm\n",
    "from imitation.util.util import make_vec_env\n",
    "from imitation.data.types import TrajectoryWithRew\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.ppo import MlpPolicy\n",
    "\n",
    "from imitation.algorithms.adversarial.gail import GAIL\n",
    "from imitation.data import rollout\n",
    "from imitation.data.wrappers import RolloutInfoWrapper\n",
    "from imitation.policies.serialize import load_policy\n",
    "from imitation.rewards.reward_nets import BasicRewardNet\n",
    "from imitation.util.networks import RunningNorm\n",
    "from imitation.util.util import make_vec_env\n",
    "\n",
    "from stk_actor.wrappers import PreprocessObservationWrapper\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "from stable_baselines3 import PPO, A2C\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "import gymnasium as gym\n",
    "from pystk2_gymnasium import AgentSpec\n",
    "from bbrl.agents.gymnasium import ParallelGymAgent, make_env\n",
    "from functools import partial\n",
    "\n",
    "noob_vec_env = make_vec_env(\"supertuxkart/flattened_multidiscrete-v0\", seed=0, n_envs=21, wrapper_class=lambda x : (PreprocessObservationWrapper(x, ret_dict=False)), env_kwargs={\n",
    "    'render_mode':None, 'agent':AgentSpec(use_ai=False, name=\"walid\"), 'laps':3,\n",
    "})\n",
    "\n",
    "tracks = ['abyss',\n",
    " 'black_forest',\n",
    " 'candela_city',\n",
    " 'cocoa_temple',\n",
    " 'cornfield_crossing',\n",
    " 'fortmagma',\n",
    " 'gran_paradiso_island',\n",
    " 'hacienda',\n",
    " 'lighthouse',\n",
    " 'mines',\n",
    " 'minigolf',\n",
    " 'olivermath',\n",
    " 'ravenbridge_mansion',\n",
    " 'sandtrack',\n",
    " 'scotland',\n",
    " 'snowmountain',\n",
    " 'snowtuxpeak',\n",
    " 'stk_enterprise',\n",
    " 'volcano_island',\n",
    " 'xr591',\n",
    " 'zengarden']\n",
    "\n",
    "for i, venv in enumerate((noob_vec_env.envs)):\n",
    "    venv.env.default_track = tracks[i%len(tracks)]\n",
    "\n",
    "# expert = load_policy(\n",
    "#     \"ppo-huggingface\",\n",
    "#     organization=\"HumanCompatibleAI\",\n",
    "#     env_name=\"seals-CartPole-v0\",\n",
    "#     venv=noob_vec_env,\n",
    "# )\n",
    "\n",
    "# rollouts = rollout.rollout(\n",
    "#     expert,\n",
    "#     expert_vec_env,\n",
    "#     rollout.make_sample_until(min_episodes=60),\n",
    "#     rng=np.random.default_rng(SEED),\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..:: Antarctica Rendering Engine 2.0 ::..\n",
      "..:: Antarctica Rendering Engine 2.0 ::..\n",
      "..:: Antarctica Rendering Engine 2.0 ::..\n",
      "..:: Antarctica Rendering Engine 2.0 ::..\n",
      "..:: Antarctica Rendering Engine 2.0 ::..\n",
      "..:: Antarctica Rendering Engine 2.0 ::..\n",
      "..:: Antarctica Rendering Engine 2.0 ::..\n",
      "..:: Antarctica Rendering Engine 2.0 ::..\n",
      "..:: Antarctica Rendering Engine 2.0 ::..\n",
      "..:: Antarctica Rendering Engine 2.0 ::..\n",
      "..:: Antarctica Rendering Engine 2.0 ::..\n",
      "..:: Antarctica Rendering Engine 2.0 ::..\n",
      "..:: Antarctica Rendering Engine 2.0 ::..\n",
      "..:: Antarctica Rendering Engine 2.0 ::..\n",
      "..:: Antarctica Rendering Engine 2.0 ::..\n",
      "..:: Antarctica Rendering Engine 2.0 ::..\n",
      "..:: Antarctica Rendering Engine 2.0 ::..\n",
      "..:: Antarctica Rendering Engine 2.0 ::..\n",
      "..:: Antarctica Rendering Engine 2.0 ::..\n",
      "..:: Antarctica Rendering Engine 2.0 ::..\n"
     ]
    }
   ],
   "source": [
    "from stk_actor.replay_buffer import SACRolloutBuffer\n",
    "import joblib, torch \n",
    "\n",
    "buffer1 = joblib.load('all_tracks_buffer_steps_2mil')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import dataclasses\n",
    "from typing import Optional, List, Dict, Any, NamedTuple\n",
    "import torch\n",
    "\n",
    "def get_trajectories_from_buffer(buffer: SACRolloutBuffer) -> List[TrajectoryWithRew]:\n",
    "    \"\"\"\n",
    "    Convert buffer data into a list of TrajectoryWithRew objects.\n",
    "    Each trajectory represents a complete episode or episode fragment.\n",
    "    \n",
    "    Args:\n",
    "        buffer: SACRolloutBuffer containing the transitions\n",
    "        \n",
    "    Returns:\n",
    "        List of TrajectoryWithRew objects\n",
    "    \"\"\"\n",
    "    trajectories = []\n",
    "    current_trajectory = {\n",
    "        'obs': [],\n",
    "        'acts': [],\n",
    "        'rews': [],\n",
    "        'infos': [],\n",
    "        'steps': []\n",
    "    }\n",
    "    \n",
    "    # Convert buffer data to numpy for processing\n",
    "    observations = buffer.observations.numpy()\n",
    "    actions = np.stack([act.numpy() for act in buffer.actions], axis=1)\n",
    "    rewards = buffer.rewards.numpy()\n",
    "    dones = buffer.dones.numpy()\n",
    "    steps = buffer.steps.numpy()\n",
    "    next_observations = buffer.next_observations.numpy()\n",
    "    \n",
    "    # Process each transition in the buffer\n",
    "    for i in range(buffer.size):\n",
    "        # Check if this is the start of a new episode (step == 0)\n",
    "        if steps[i][0] == 0 and len(current_trajectory['obs']) > 0:\n",
    "            print(i, steps[i][0],)\n",
    "            # Convert current trajectory to TrajectoryWithRew and add to list\n",
    "            current_trajectory['obs'].append(next_observations[i-1])\n",
    "            trajectories.append(_create_trajectory(\n",
    "                current_trajectory['obs'],\n",
    "                current_trajectory['acts'],\n",
    "                current_trajectory['rews'],\n",
    "                current_trajectory['infos'],\n",
    "                True  # Terminal since it's end of episode\n",
    "            ))\n",
    "            \n",
    "            # Reset current trajectory\n",
    "            current_trajectory = {\n",
    "                'obs': [],\n",
    "                'acts': [],\n",
    "                'rews': [],\n",
    "                'infos': [],\n",
    "                'steps': []\n",
    "            }\n",
    "            \n",
    "        # Add current transition to trajectory\n",
    "        current_trajectory['obs'].append(observations[i])\n",
    "        current_trajectory['acts'].append(actions[i])\n",
    "        current_trajectory['rews'].append(rewards[i])\n",
    "        current_trajectory['infos'].append({\n",
    "            'step': steps[i],\n",
    "            'done': False\n",
    "        })\n",
    "        current_trajectory['steps'].append(steps[i])\n",
    "        \n",
    "        # If this is the last transition or it's done, create a trajectory\n",
    "        if i == buffer.size - 1:\n",
    "            \n",
    "            if len(current_trajectory['obs']) > 0:\n",
    "                # Add final observation\n",
    "                if i + 1 < buffer.size:\n",
    "                    current_trajectory['obs'].append(observations[i + 1])\n",
    "                else:\n",
    "                    current_trajectory['obs'].append(observations[i])  # Use last observation if at buffer end\n",
    "                    \n",
    "                trajectories.append(_create_trajectory(\n",
    "                    current_trajectory['obs'],\n",
    "                    current_trajectory['acts'],\n",
    "                    current_trajectory['rews'],\n",
    "                    current_trajectory['infos'],\n",
    "                    True\n",
    "                ))\n",
    "                # Reset current trajectory\n",
    "                current_trajectory = {\n",
    "                    'obs': [],\n",
    "                    'acts': [],\n",
    "                    'rews': [],\n",
    "                    'infos': [],\n",
    "                    'steps': []\n",
    "                }\n",
    "\n",
    "    return trajectories\n",
    "\n",
    "def _create_trajectory(\n",
    "    observations: List[np.ndarray],\n",
    "    actions: List[np.ndarray],\n",
    "    rewards: List[np.ndarray],\n",
    "    infos: List[Dict[str, Any]],\n",
    "    terminal: bool\n",
    ") -> TrajectoryWithRew:\n",
    "    \"\"\"\n",
    "    Create a TrajectoryWithRew object from lists of transitions.\n",
    "    \n",
    "    Args:\n",
    "        observations: List of observations including final observation\n",
    "        actions: List of actions\n",
    "        rewards: List of rewards\n",
    "        infos: List of info dictionaries\n",
    "        terminal: Whether trajectory ends in terminal state\n",
    "        \n",
    "    Returns:\n",
    "        TrajectoryWithRew object\n",
    "    \"\"\"\n",
    "    return TrajectoryWithRew(\n",
    "        obs=np.stack(observations),\n",
    "        acts=np.stack(actions),\n",
    "        rews=np.stack(rewards).flatten(),\n",
    "        infos=np.array(infos),\n",
    "        terminal=terminal\n",
    "    )\n",
    "\n",
    "def get_trajectories_by_track(\n",
    "    buffer: SACRolloutBuffer,\n",
    "    track_name: str\n",
    ") -> List[TrajectoryWithRew]:\n",
    "    \"\"\"\n",
    "    Get trajectories for a specific track from the buffer.\n",
    "    \n",
    "    Args:\n",
    "        buffer: SACRolloutBuffer containing the transitions\n",
    "        track_name: Name of the track to filter trajectories for\n",
    "        \n",
    "    Returns:\n",
    "        List of TrajectoryWithRew objects for the specified track\n",
    "    \"\"\"\n",
    "    track_id = buffer.tracks[track_name]\n",
    "    track_mask = buffer.track.numpy().flatten() == track_id\n",
    "    \n",
    "    # Create a new temporary buffer with only the track data\n",
    "    temp_buffer = SACRolloutBuffer(\n",
    "        buffer_size=buffer.buffer_size,\n",
    "        obs_dim=buffer.obs_dim,\n",
    "        action_dims=buffer.action_dims\n",
    "    )\n",
    "    \n",
    "    # Copy only the track data\n",
    "    temp_buffer.observations = buffer.observations[track_mask]\n",
    "    temp_buffer.actions = [act[track_mask] for act in buffer.actions]\n",
    "    temp_buffer.rewards = buffer.rewards[track_mask]\n",
    "    temp_buffer.next_observations = buffer.next_observations[track_mask]\n",
    "    temp_buffer.dones = buffer.dones[track_mask]\n",
    "    temp_buffer.steps = buffer.steps[track_mask]\n",
    "    temp_buffer.track = buffer.track[track_mask]\n",
    "    temp_buffer.size = track_mask.sum()\n",
    "    \n",
    "    return get_trajectories_from_buffer(temp_buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2871 0.0\n",
      "8764 0.0\n",
      "11182 0.0\n",
      "13913 0.0\n",
      "16844 0.0\n",
      "19281 0.0\n",
      "22393 0.0\n",
      "25023 0.0\n",
      "26873 0.0\n",
      "29509 0.0\n",
      "31222 0.0\n",
      "32247 0.0\n",
      "35328 0.0\n",
      "38023 0.0\n",
      "40714 0.0\n",
      "43290 0.0\n",
      "45436 0.0\n",
      "48577 0.0\n",
      "53313 0.0\n",
      "56087 0.0\n",
      "57575 0.0\n",
      "59483 0.0\n",
      "63249 0.0\n",
      "64792 0.0\n",
      "66589 0.0\n",
      "68506 0.0\n",
      "70078 0.0\n",
      "72056 0.0\n",
      "73768 0.0\n",
      "75066 0.0\n",
      "76800 0.0\n",
      "78016 0.0\n",
      "78798 0.0\n",
      "80884 0.0\n",
      "82601 0.0\n",
      "84393 0.0\n",
      "85986 0.0\n",
      "87402 0.0\n",
      "89409 0.0\n",
      "92467 0.0\n",
      "94302 0.0\n",
      "95366 0.0\n",
      "97020 0.0\n",
      "100531 0.0\n",
      "101905 0.0\n",
      "103579 0.0\n",
      "105259 0.0\n",
      "106693 0.0\n",
      "108564 0.0\n",
      "110121 0.0\n",
      "111213 0.0\n",
      "112805 0.0\n",
      "113931 0.0\n",
      "114612 0.0\n",
      "116455 0.0\n",
      "118015 0.0\n",
      "119590 0.0\n",
      "121027 0.0\n",
      "122352 0.0\n",
      "124285 0.0\n",
      "127091 0.0\n",
      "128809 0.0\n",
      "129766 0.0\n",
      "132632 0.0\n",
      "138414 0.0\n",
      "140773 0.0\n",
      "143489 0.0\n",
      "146382 0.0\n",
      "148859 0.0\n",
      "152017 0.0\n",
      "154648 0.0\n",
      "156532 0.0\n",
      "159178 0.0\n",
      "160921 0.0\n",
      "161955 0.0\n",
      "165025 0.0\n",
      "167795 0.0\n",
      "170494 0.0\n",
      "173087 0.0\n",
      "175256 0.0\n",
      "178385 0.0\n",
      "183115 0.0\n",
      "185866 0.0\n",
      "187325 0.0\n",
      "189200 0.0\n",
      "193037 0.0\n",
      "194773 0.0\n",
      "196486 0.0\n",
      "198380 0.0\n",
      "199930 0.0\n",
      "201968 0.0\n",
      "203700 0.0\n",
      "204986 0.0\n",
      "206713 0.0\n",
      "207882 0.0\n",
      "208683 0.0\n",
      "210668 0.0\n",
      "212387 0.0\n",
      "214087 0.0\n",
      "215696 0.0\n",
      "217073 0.0\n",
      "219070 0.0\n",
      "222209 0.0\n",
      "224018 0.0\n",
      "225015 0.0\n",
      "226792 0.0\n",
      "230274 0.0\n",
      "231687 0.0\n",
      "233280 0.0\n",
      "235004 0.0\n",
      "236382 0.0\n",
      "238233 0.0\n",
      "239781 0.0\n",
      "240912 0.0\n",
      "242480 0.0\n",
      "243623 0.0\n",
      "244279 0.0\n",
      "246089 0.0\n",
      "247642 0.0\n",
      "249175 0.0\n",
      "250624 0.0\n",
      "251895 0.0\n",
      "253795 0.0\n",
      "256569 0.0\n",
      "258279 0.0\n",
      "259232 0.0\n",
      "262169 0.0\n",
      "268054 0.0\n",
      "270407 0.0\n",
      "273109 0.0\n",
      "276009 0.0\n",
      "278431 0.0\n",
      "281550 0.0\n",
      "284198 0.0\n",
      "286116 0.0\n",
      "288733 0.0\n",
      "290399 0.0\n",
      "291424 0.0\n",
      "294478 0.0\n",
      "297242 0.0\n",
      "299940 0.0\n",
      "302514 0.0\n",
      "304711 0.0\n",
      "307784 0.0\n",
      "312461 0.0\n",
      "315299 0.0\n",
      "316885 0.0\n",
      "318789 0.0\n",
      "322592 0.0\n",
      "324139 0.0\n",
      "325837 0.0\n",
      "327717 0.0\n",
      "329434 0.0\n",
      "331545 0.0\n",
      "333237 0.0\n",
      "334462 0.0\n",
      "336154 0.0\n",
      "337399 0.0\n",
      "338127 0.0\n",
      "340112 0.0\n",
      "341815 0.0\n",
      "343558 0.0\n",
      "345177 0.0\n",
      "346587 0.0\n",
      "348566 0.0\n",
      "351620 0.0\n",
      "353477 0.0\n",
      "354565 0.0\n",
      "356243 0.0\n",
      "359735 0.0\n",
      "361119 0.0\n",
      "362752 0.0\n",
      "364381 0.0\n",
      "365795 0.0\n",
      "367681 0.0\n",
      "369227 0.0\n",
      "370347 0.0\n",
      "371911 0.0\n",
      "373131 0.0\n",
      "373804 0.0\n",
      "375680 0.0\n",
      "377282 0.0\n",
      "378857 0.0\n",
      "380326 0.0\n",
      "381622 0.0\n",
      "383537 0.0\n",
      "386265 0.0\n",
      "388040 0.0\n",
      "389018 0.0\n",
      "391940 0.0\n",
      "397834 0.0\n",
      "400194 0.0\n",
      "402897 0.0\n",
      "405834 0.0\n",
      "408251 0.0\n",
      "411441 0.0\n",
      "414121 0.0\n",
      "415990 0.0\n",
      "418607 0.0\n",
      "420308 0.0\n",
      "421335 0.0\n",
      "424327 0.0\n",
      "427026 0.0\n",
      "429703 0.0\n",
      "432255 0.0\n",
      "434432 0.0\n",
      "437541 0.0\n",
      "442260 0.0\n",
      "445058 0.0\n",
      "446630 0.0\n",
      "448490 0.0\n",
      "452285 0.0\n",
      "453932 0.0\n",
      "455696 0.0\n",
      "457554 0.0\n",
      "459253 0.0\n",
      "461325 0.0\n",
      "463011 0.0\n",
      "464259 0.0\n",
      "466017 0.0\n",
      "467179 0.0\n",
      "467967 0.0\n",
      "469960 0.0\n",
      "471657 0.0\n",
      "473317 0.0\n",
      "474961 0.0\n",
      "476359 0.0\n",
      "478381 0.0\n",
      "481494 0.0\n",
      "483420 0.0\n",
      "484528 0.0\n",
      "486169 0.0\n",
      "489620 0.0\n",
      "491112 0.0\n",
      "492722 0.0\n",
      "494406 0.0\n",
      "495869 0.0\n",
      "497710 0.0\n",
      "499254 0.0\n",
      "500394 0.0\n",
      "501976 0.0\n",
      "503135 0.0\n",
      "503790 0.0\n",
      "505578 0.0\n",
      "507119 0.0\n",
      "508684 0.0\n",
      "510173 0.0\n",
      "511503 0.0\n",
      "513345 0.0\n",
      "516034 0.0\n",
      "517767 0.0\n",
      "518744 0.0\n",
      "521596 0.0\n",
      "527508 0.0\n",
      "529894 0.0\n",
      "532620 0.0\n",
      "535549 0.0\n",
      "537974 0.0\n",
      "541120 0.0\n",
      "543735 0.0\n",
      "545590 0.0\n",
      "548196 0.0\n",
      "549928 0.0\n",
      "550998 0.0\n",
      "554054 0.0\n",
      "556773 0.0\n",
      "559445 0.0\n",
      "562047 0.0\n",
      "564240 0.0\n",
      "567381 0.0\n",
      "572067 0.0\n",
      "574869 0.0\n",
      "576404 0.0\n",
      "578342 0.0\n",
      "582066 0.0\n",
      "583715 0.0\n",
      "585481 0.0\n",
      "587339 0.0\n",
      "588887 0.0\n",
      "590950 0.0\n",
      "592678 0.0\n",
      "593913 0.0\n",
      "595670 0.0\n",
      "596897 0.0\n",
      "597620 0.0\n",
      "599615 0.0\n",
      "601322 0.0\n",
      "603086 0.0\n",
      "604632 0.0\n",
      "606056 0.0\n",
      "608158 0.0\n",
      "611229 0.0\n",
      "613014 0.0\n",
      "614071 0.0\n",
      "615750 0.0\n",
      "619207 0.0\n",
      "620624 0.0\n",
      "622218 0.0\n",
      "623895 0.0\n",
      "625322 0.0\n",
      "627158 0.0\n",
      "628699 0.0\n",
      "629827 0.0\n",
      "631407 0.0\n",
      "632533 0.0\n",
      "633250 0.0\n",
      "635107 0.0\n",
      "636641 0.0\n",
      "638268 0.0\n",
      "639717 0.0\n",
      "641019 0.0\n",
      "642851 0.0\n",
      "645595 0.0\n",
      "647381 0.0\n"
     ]
    }
   ],
   "source": [
    "rollouts = get_trajectories_from_buffer(buffer1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "315"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rollouts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5b/wsyz4crx22bg_7tpsgbw3tzw0000gn/T/ipykernel_64908/3874999844.py:29: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  unified_policy.load_state_dict(torch.load(filename))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from stk_actor.agent import UnifiedSACPolicy\n",
    "\n",
    "\n",
    "net_arch=[512,512,512,512]\n",
    "activation_fn=torch.nn.SiLU\n",
    "filename = 'policy_512_512_512_512_SiLU_3_statedict'\n",
    "\n",
    "learner = PPO(\n",
    "    env=noob_vec_env,\n",
    "    policy=MlpPolicy,\n",
    "    batch_size=64,\n",
    "    ent_coef=0.0,\n",
    "    learning_rate=0.0005,\n",
    "    gamma=0.95,\n",
    "    clip_range=0.1,\n",
    "    vf_coef=0.1,\n",
    "    n_epochs=5,\n",
    "    seed=SEED,\n",
    "    policy_kwargs=dict(net_arch=net_arch, activation_fn=activation_fn,), \n",
    ")\n",
    "\n",
    "action_dims = [space.n for space in noob_vec_env.action_space]\n",
    "unified_policy = UnifiedSACPolicy(\n",
    "    noob_vec_env.observation_space, \n",
    "    action_dims, \n",
    "    net_arch=net_arch, \n",
    "    activation_fn=activation_fn\n",
    ")\n",
    "unified_policy.load_state_dict(torch.load(filename))\n",
    "learner.policy.load_state_dict(unified_policy.shared.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# reward_net = BasicShapedRewardNet(\n",
    "#     observation_space=noob_vec_env.observation_space,\n",
    "#     action_space=noob_vec_env.action_space,\n",
    "#     normalize_input_layer=RunningNorm,\n",
    "# )\n",
    "# airl_trainer = AIRL(\n",
    "#     demonstrations=rollouts,\n",
    "#     demo_batch_size=2048,\n",
    "#     gen_replay_buffer_capacity=512,\n",
    "#     n_disc_updates_per_round=16,\n",
    "#     venv=noob_vec_env,\n",
    "#     gen_algo=learner,\n",
    "#     reward_net=reward_net,\n",
    "#     allow_variable_horizon=True\n",
    "# )\n",
    "\n",
    "\n",
    "density_trainer = db.DensityAlgorithm(\n",
    "    venv=noob_vec_env,\n",
    "    rng=np.random.default_rng(0),\n",
    "    demonstrations=rollouts,\n",
    "    rl_algo=learner,\n",
    "    density_type=db.DensityType.STATE_ACTION_DENSITY,\n",
    "    is_stationary=True,\n",
    "    kernel=\"gaussian\",\n",
    "    kernel_bandwidth=0.4,\n",
    "    standardise_inputs=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "density_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "\n",
    "\n",
    "def print_stats(density_trainer, n_trajectories):\n",
    "    stats = density_trainer.test_policy(n_trajectories=n_trajectories)\n",
    "    print(\"True reward function stats:\")\n",
    "    pprint.pprint(stats)\n",
    "    stats_im = density_trainer.test_policy(true_reward=False, n_trajectories=n_trajectories)\n",
    "    print(\"Imitation reward function stats:\")\n",
    "    pprint.pprint(stats_im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Stats before training:\")\n",
    "print_stats(density_trainer, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Episodes of different length detected: {1040, 1500, 1231}. Variable horizon environments are discouraged -- termination conditions leak information about reward. See https://imitation.readthedocs.io/en/latest/getting-started/variable-horizon.html for more information. If you are SURE you want to run imitation on a variable horizon task, then please pass in the flag: `allow_variable_horizon=True`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdensity_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_policy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Train for 1_000_000 steps to approach expert performance.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/bbrl/lib/python3.10/site-packages/imitation/algorithms/density.py:382\u001b[0m, in \u001b[0;36mDensityAlgorithm.train_policy\u001b[0;34m(self, n_timesteps, **kwargs)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrl_algo\u001b[38;5;241m.\u001b[39mlearn(\n\u001b[1;32m    374\u001b[0m     n_timesteps,\n\u001b[1;32m    375\u001b[0m     \u001b[38;5;66;03m# ensure we can see total steps for all\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    379\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    380\u001b[0m )\n\u001b[1;32m    381\u001b[0m trajs, ep_lens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffering_wrapper\u001b[38;5;241m.\u001b[39mpop_trajectories()\n\u001b[0;32m--> 382\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_fixed_horizon\u001b[49m\u001b[43m(\u001b[49m\u001b[43mep_lens\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/bbrl/lib/python3.10/site-packages/imitation/algorithms/base.py:99\u001b[0m, in \u001b[0;36mBaseImitationAlgorithm._check_fixed_horizon\u001b[0;34m(self, horizons)\u001b[0m\n\u001b[1;32m     96\u001b[0m     horizons\u001b[38;5;241m.\u001b[39madd(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_horizon)\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(horizons) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 99\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    100\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpisodes of different length detected: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhorizons\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    101\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVariable horizon environments are discouraged -- \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    102\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtermination conditions leak information about reward. See \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    103\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://imitation.readthedocs.io/en/latest/getting-started/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    104\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvariable-horizon.html for more information. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    105\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf you are SURE you want to run imitation on a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    106\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvariable horizon task, then please pass in the flag: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    107\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`allow_variable_horizon=True`.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    108\u001b[0m     )\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(horizons) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_horizon \u001b[38;5;241m=\u001b[39m horizons\u001b[38;5;241m.\u001b[39mpop()\n",
      "\u001b[0;31mValueError\u001b[0m: Episodes of different length detected: {1040, 1500, 1231}. Variable horizon environments are discouraged -- termination conditions leak information about reward. See https://imitation.readthedocs.io/en/latest/getting-started/variable-horizon.html for more information. If you are SURE you want to run imitation on a variable horizon task, then please pass in the flag: `allow_variable_horizon=True`."
     ]
    }
   ],
   "source": [
    "density_trainer.train_policy(100)  # Train for 1_000_000 steps to approach expert performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Stats after training:\")\n",
    "print_stats(density_trainer, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bbrl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
