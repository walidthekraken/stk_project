{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "making 21 environments\n",
      "..:: Antarctica Rendering Engine 2.0 ::..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5b/wsyz4crx22bg_7tpsgbw3tzw0000gn/T/ipykernel_42946/2426701993.py:20: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.mean = torch.load(path+f'{agent}/buffer_mean', map_location='cpu')\n",
      "/var/folders/5b/wsyz4crx22bg_7tpsgbw3tzw0000gn/T/ipykernel_42946/2426701993.py:21: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.std = torch.load(path+f'{agent}/buffer_std', map_location='cpu')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 (0, 'abyss')\n",
      "1 (1, 'black_forest')\n",
      "2 (2, 'candela_city')\n",
      "3 (3, 'cocoa_temple')\n",
      "4 (4, 'cornfield_crossing')\n",
      "5 (5, 'fortmagma')\n",
      "6 (6, 'gran_paradiso_island')\n",
      "7 (7, 'hacienda')\n",
      "8 (8, 'lighthouse')\n",
      "9 (9, 'mines')\n",
      "10 (10, 'minigolf')\n",
      "11 (11, 'olivermath')\n",
      "12 (12, 'ravenbridge_mansion')\n",
      "13 (13, 'sandtrack')\n",
      "14 (14, 'scotland')\n",
      "15 (15, 'snowmountain')\n",
      "16 (16, 'snowtuxpeak')\n",
      "17 (17, 'stk_enterprise')\n",
      "18 (18, 'volcano_island')\n",
      "19 (19, 'xr591')\n",
      "20 (20, 'zengarden')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5b/wsyz4crx22bg_7tpsgbw3tzw0000gn/T/ipykernel_42946/2426701993.py:324: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  unified_policy.load_state_dict(torch.load(filename, map_location='cpu'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from stk_actor.wrappers import StuckStopWrapper\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "path = 'stk_actor/trained_agents/'\n",
    "agent = 'normed_a2c_num5_best'\n",
    "\n",
    "class PreprocessObservationWrapper(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        \"\"\"\n",
    "        A Gym wrapper to preprocess mixed observation space (continuous + discrete)\n",
    "        into a flat tensor.\n",
    "        \n",
    "        Args:\n",
    "            env: The Gym environment to wrap.\n",
    "        \"\"\"\n",
    "        super().__init__(env)\n",
    "        self.observation_space = self._get_flat_observation_space(env.observation_space)\n",
    "        self.mean = torch.load(path+f'{agent}/buffer_mean', map_location='cpu')\n",
    "        self.std = torch.load(path+f'{agent}/buffer_std', map_location='cpu')\n",
    "\n",
    "    def _get_flat_observation_space(self, observation_space):\n",
    "        \"\"\"\n",
    "        Create a flat observation space based on the original observation space.\n",
    "        \n",
    "        Args:\n",
    "            observation_space: Original observation space with 'continuous' and 'discrete' components.\n",
    "        \n",
    "        Returns:\n",
    "            A flattened observation space.\n",
    "        \"\"\"\n",
    "        continuous_dim = observation_space['continuous'].shape[0]\n",
    "        discrete_dims = sum(space.n for space in observation_space['discrete'])\n",
    "        flat_dim = continuous_dim + discrete_dims\n",
    "        return gym.spaces.Box(low=-float('inf'), high=float('inf'), shape=(flat_dim,), dtype=float)\n",
    "\n",
    "    def observation(self, obs):\n",
    "        \"\"\"\n",
    "        Process the observation into a flat tensor.\n",
    "        \n",
    "        Args:\n",
    "            obs: The raw observation from the environment.\n",
    "        \n",
    "        Returns:\n",
    "            A preprocessed flat tensor.\n",
    "        \"\"\"\n",
    "        continuous_obs, discrete_obs = obs['continuous'], obs['discrete']\n",
    "        continuous_tensor = torch.FloatTensor(continuous_obs)\n",
    "        \n",
    "        discrete_tensors = [\n",
    "            F.one_hot(torch.tensor(x), num_classes=num_classes.n).float()\n",
    "            for x, num_classes in zip(discrete_obs, self.env.observation_space['discrete'])\n",
    "        ]\n",
    "        \n",
    "        flat_tensor = torch.cat([continuous_tensor] + discrete_tensors)\n",
    "        normed_flat_tensor = (flat_tensor - self.mean) / (self.std + 1e-8)\n",
    "        return normed_flat_tensor\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium import Wrapper\n",
    "\n",
    "class SkipFirstNStepsWrapper(Wrapper):\n",
    "    def __init__(self, env, n):\n",
    "        super().__init__(env)\n",
    "        self.n = n\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        # Reset the environment\n",
    "        obs, info = self.env.reset(**kwargs)\n",
    "        # Skip the first n steps\n",
    "        for _ in range(self.n):\n",
    "            obs, _, done, truncated, info = self.env.step(self.env.action_space.sample())\n",
    "            if done or truncated:\n",
    "                obs, info = self.env.reset(**kwargs)\n",
    "        return obs, info\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "from typing import Dict, List, Tuple, Union, Type\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "\n",
    "def get_device(device: Union[torch.device, str] = \"auto\") -> torch.device:\n",
    "    if device == \"auto\":\n",
    "        device = \"cuda\"\n",
    "    device = torch.device(device)\n",
    "    if device.type == torch.device(\"cuda\").type and not torch.cuda.is_available():\n",
    "        return torch.device(\"cpu\")\n",
    "    return device\n",
    "\n",
    "class BaseFeaturesExtractor(nn.Module):\n",
    "    def __init__(self, observation_space: gym.Space, features_dim: int = 0) -> None:\n",
    "        super().__init__()\n",
    "        assert features_dim > 0\n",
    "        self._observation_space = observation_space\n",
    "        self._features_dim = features_dim\n",
    "    @property\n",
    "    def features_dim(self) -> int:\n",
    "        return self._features_dim\n",
    "\n",
    "def get_flattened_obs_dim(observation_space: spaces.Space) -> int:\n",
    "    if isinstance(observation_space, spaces.MultiDiscrete):\n",
    "        return sum(observation_space.nvec)\n",
    "    else:\n",
    "        return spaces.utils.flatdim(observation_space)\n",
    "\n",
    "class FlattenExtractor(BaseFeaturesExtractor):\n",
    "    def __init__(self, observation_space: gym.Space) -> None:\n",
    "        super().__init__(observation_space, get_flattened_obs_dim(observation_space))\n",
    "        self.flatten = nn.Flatten()\n",
    "    def forward(self, observations: torch.Tensor) -> torch.Tensor:\n",
    "        return self.flatten(observations)\n",
    "    \n",
    "class MlpExtractor(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        feature_dim: int,\n",
    "        net_arch: Union[List[int], Dict[str, List[int]]],\n",
    "        activation_fn: Type[nn.Module],\n",
    "        device: Union[torch.device, str] = \"auto\",\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        # device = torch.get_device(device)\n",
    "        policy_net: List[nn.Module] = []\n",
    "        value_net: List[nn.Module] = []\n",
    "        last_layer_dim_pi = feature_dim\n",
    "        last_layer_dim_vf = feature_dim\n",
    "\n",
    "        if isinstance(net_arch, dict):\n",
    "            pi_layers_dims = net_arch.get(\"pi\", []) \n",
    "            vf_layers_dims = net_arch.get(\"vf\", []) \n",
    "        else:\n",
    "            pi_layers_dims = vf_layers_dims = net_arch\n",
    "        for curr_layer_dim in pi_layers_dims:\n",
    "            policy_net.append(nn.Linear(last_layer_dim_pi, curr_layer_dim))\n",
    "            policy_net.append(activation_fn())\n",
    "            last_layer_dim_pi = curr_layer_dim\n",
    "        for curr_layer_dim in vf_layers_dims:\n",
    "            value_net.append(nn.Linear(last_layer_dim_vf, curr_layer_dim))\n",
    "            value_net.append(activation_fn())\n",
    "            last_layer_dim_vf = curr_layer_dim\n",
    "\n",
    "        self.latent_dim_pi = last_layer_dim_pi\n",
    "        self.latent_dim_vf = last_layer_dim_vf\n",
    "        self.policy_net = nn.Sequential(*policy_net)#.to(device)\n",
    "        self.value_net = nn.Sequential(*value_net)#.to(device)\n",
    "\n",
    "    def forward(self, features: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        :return: latent_policy, latent_value of the specified network.\n",
    "            If all layers are shared, then ``latent_policy == latent_value``\n",
    "        \"\"\"\n",
    "        return self.forward_actor(features), self.forward_critic(features)\n",
    "\n",
    "    def forward_actor(self, features: torch.Tensor) -> torch.Tensor:\n",
    "        return self.policy_net(features)\n",
    "\n",
    "    def forward_critic(self, features: torch.Tensor) -> torch.Tensor:\n",
    "        return self.value_net(features)\n",
    "\n",
    "    \n",
    "class Policy(nn.Module):\n",
    "    def __init__(self, observation_space, action_dims, net_arch, activation_fn,):\n",
    "        super().__init__()\n",
    "        self.features_extractor = FlattenExtractor(observation_space)\n",
    "        self.pi_features_extractor = self.features_extractor\n",
    "        self.vf_features_extractor = self.features_extractor\n",
    "        self.mlp_extractor = MlpExtractor(\n",
    "            self.features_extractor.features_dim,\n",
    "            net_arch=net_arch,\n",
    "            activation_fn=activation_fn,\n",
    "        )\n",
    "        self.action_net = nn.Linear(net_arch[-1], sum(action_dims))\n",
    "        self.value_net = nn.Linear(net_arch[-1], 1)\n",
    "\n",
    "\n",
    "class UnifiedSACPolicy(nn.Module):\n",
    "    def __init__(self, observation_space, action_dims, net_arch, activation_fn):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.shared = Policy(\n",
    "            observation_space,\n",
    "            action_dims,\n",
    "            net_arch=net_arch,\n",
    "            activation_fn=activation_fn\n",
    "        )\n",
    "        self.action_dims = action_dims\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.shared.features_extractor(x)\n",
    "        x = self.shared.mlp_extractor.policy_net(x)\n",
    "        x = self.shared.action_net(x)\n",
    "        return x\n",
    "    \n",
    "    def sample(self, x, deterministic=False):\n",
    "        logits = self.forward(x)\n",
    "        \n",
    "        # Split logits for each action dimension\n",
    "        split_logits = torch.split(logits, self.action_dims, dim=-1)\n",
    "        \n",
    "        actions = []\n",
    "        log_probs = []\n",
    "        probs = []\n",
    "        \n",
    "        for logit in split_logits:\n",
    "            distribution = Categorical(logits=logit)\n",
    "            if deterministic:\n",
    "                action = torch.argmax(logit, dim=-1)\n",
    "            else:\n",
    "                action = distribution.sample()\n",
    "            \n",
    "            log_prob = distribution.log_prob(action)\n",
    "            prob = F.softmax(logit, dim=-1)\n",
    "            \n",
    "            actions.append(action)\n",
    "            log_probs.append(log_prob)\n",
    "            probs.append(prob)\n",
    "        \n",
    "        return (\n",
    "            torch.stack(actions),\n",
    "            torch.stack(log_probs),\n",
    "            probs\n",
    "        )\n",
    "    \n",
    "#policy = torch.load('policy_512_512_512_512_SiLU_3_statedict', map_location='cuda')\n",
    "\n",
    "\n",
    "from stable_baselines3 import PPO, A2C\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "import gymnasium as gym\n",
    "from pystk2_gymnasium import AgentSpec\n",
    "from bbrl.agents.gymnasium import ParallelGymAgent, make_env\n",
    "from functools import partial\n",
    "\n",
    "tracks = [\n",
    "    'abyss',\n",
    "    'black_forest',\n",
    "    'candela_city',\n",
    "    'cocoa_temple',\n",
    "    'cornfield_crossing',\n",
    "    'fortmagma',\n",
    "    'gran_paradiso_island',\n",
    "    'hacienda',\n",
    "    'lighthouse',\n",
    "    'mines',\n",
    "    'minigolf',\n",
    "    'olivermath',\n",
    "    'ravenbridge_mansion',\n",
    "    'sandtrack',\n",
    "    'scotland',\n",
    "    'snowmountain',\n",
    "    'snowtuxpeak',\n",
    "    'stk_enterprise',\n",
    "    'volcano_island',\n",
    "    'xr591',\n",
    "    'zengarden',\n",
    "\n",
    "# # #   ==================   #\n",
    "\n",
    "#     'fortmagma',\n",
    "#     'ravenbridge_mansion',\n",
    "#     'snowmountain',\n",
    "#     'cocoa_temple',\n",
    "#     'sandtrack',    \n",
    "#     'scotland', \n",
    "#     'stk_enterprise',\n",
    "#     'volcano_island', # 1104\n",
    "#     'xr591', # 864   \n",
    "]\n",
    "\n",
    "\n",
    "karts = [12]\n",
    "n_envs = len(tracks)*len(karts)\n",
    "\n",
    "print('making', n_envs, 'environments')\n",
    "vec_env = make_vec_env(\n",
    "    \"supertuxkart/flattened_multidiscrete-v0\",\n",
    "    # seed=12,\n",
    "    n_envs=n_envs, \n",
    "    wrapper_class=lambda x : (\n",
    "        SkipFirstNStepsWrapper(\n",
    "            StuckStopWrapper(\n",
    "                PreprocessObservationWrapper(x),\n",
    "                n=92,\n",
    "            ), \n",
    "            n=20,\n",
    "        )\n",
    "    ), \n",
    "    env_kwargs={\n",
    "    'render_mode':None, 'agent':AgentSpec(use_ai=False, name=\"walid\"), #'track':'minigolf', \n",
    "    'laps':1,\n",
    "    'difficulty':2, \n",
    "    'num_kart':12, #'difficulty':0\n",
    "})\n",
    "\n",
    "ix = 0\n",
    "for num_kart in enumerate(karts):\n",
    "    for track in enumerate(tracks):\n",
    "        venv = vec_env.envs[ix]\n",
    "        venv.env.default_track = track\n",
    "        venv.env.num_kart = num_kart\n",
    "        print(ix, track, )\n",
    "        ix+=1\n",
    "\n",
    "net_arch=[1024,1024,1024]\n",
    "activation_fn=torch.nn.Tanh\n",
    "filename = path+f'{agent}/statedict'\n",
    "\n",
    "action_dims = [space.n for space in vec_env.action_space]\n",
    "unified_policy = UnifiedSACPolicy(\n",
    "    vec_env.observation_space, \n",
    "    action_dims, \n",
    "    net_arch=net_arch, \n",
    "    activation_fn=activation_fn\n",
    ")\n",
    "unified_policy.load_state_dict(torch.load(filename, map_location='cpu'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "..:: Antarctica Rendering Engine 2.0 ::..\n",
      "..:: Antarctica Rendering Engine 2.0 ::..\n",
      "..:: Antarctica Rendering Engine 2.0 ::..\n",
      "..:: Antarctica Rendering Engine 2.0 ::..\n",
      "..:: Antarctica Rendering Engine 2.0 ::..\n",
      "..:: Antarctica Rendering Engine 2.0 ::..\n",
      "..:: Antarctica Rendering Engine 2.0 ::..\n",
      "..:: Antarctica Rendering Engine 2.0 ::..\n",
      "..:: Antarctica Rendering Engine 2.0 ::..\n",
      "..:: Antarctica Rendering Engine 2.0 ::..\n",
      "..:: Antarctica Rendering Engine 2.0 ::..\n",
      "..:: Antarctica Rendering Engine 2.0 ::..\n",
      "..:: Antarctica Rendering Engine 2.0 ::..\n",
      "..:: Antarctica Rendering Engine 2.0 ::..\n",
      "..:: Antarctica Rendering Engine 2.0 ::..\n",
      "..:: Antarctica Rendering Engine 2.0 ::..\n",
      "..:: Antarctica Rendering Engine 2.0 ::..\n",
      "..:: Antarctica Rendering Engine 2.0 ::..\n",
      "..:: Antarctica Rendering Engine 2.0 ::..\n",
      "..:: Antarctica Rendering Engine 2.0 ::..\n",
      "DOING 420 300000\n",
      "Logging to ./outputs/A2C_12\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e3deab0f4fe46beb21c5042aab1fd06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 348      |\n",
      "|    ep_rew_mean     | 458      |\n",
      "| time/              |          |\n",
      "|    fps             | 56       |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 154      |\n",
      "|    total_timesteps | 8820     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 502      |\n",
      "|    ep_rew_mean        | 520      |\n",
      "| time/                 |          |\n",
      "|    fps                | 54       |\n",
      "|    iterations         | 2        |\n",
      "|    time_elapsed       | 322      |\n",
      "|    total_timesteps    | 17640    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.222   |\n",
      "|    explained_variance | 0.14     |\n",
      "|    learning_rate      | 0.0006   |\n",
      "|    n_updates          | 1        |\n",
      "|    policy_loss        | -0.0102  |\n",
      "|    value_loss         | 2.53e+03 |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 535      |\n",
      "|    ep_rew_mean        | 499      |\n",
      "| time/                 |          |\n",
      "|    fps                | 51       |\n",
      "|    iterations         | 3        |\n",
      "|    time_elapsed       | 517      |\n",
      "|    total_timesteps    | 26460    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.244   |\n",
      "|    explained_variance | 0.264    |\n",
      "|    learning_rate      | 0.0006   |\n",
      "|    n_updates          | 2        |\n",
      "|    policy_loss        | -0.0199  |\n",
      "|    value_loss         | 1.95e+03 |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 576      |\n",
      "|    ep_rew_mean        | 508      |\n",
      "| time/                 |          |\n",
      "|    fps                | 51       |\n",
      "|    iterations         | 4        |\n",
      "|    time_elapsed       | 685      |\n",
      "|    total_timesteps    | 35280    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.241   |\n",
      "|    explained_variance | 0.372    |\n",
      "|    learning_rate      | 0.0006   |\n",
      "|    n_updates          | 3        |\n",
      "|    policy_loss        | 0.000122 |\n",
      "|    value_loss         | 1.51e+03 |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 584      |\n",
      "|    ep_rew_mean        | 516      |\n",
      "| time/                 |          |\n",
      "|    fps                | 51       |\n",
      "|    iterations         | 5        |\n",
      "|    time_elapsed       | 851      |\n",
      "|    total_timesteps    | 44100    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.237   |\n",
      "|    explained_variance | 0.368    |\n",
      "|    learning_rate      | 0.0006   |\n",
      "|    n_updates          | 4        |\n",
      "|    policy_loss        | 0.00375  |\n",
      "|    value_loss         | 1.55e+03 |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 582      |\n",
      "|    ep_rew_mean        | 507      |\n",
      "| time/                 |          |\n",
      "|    fps                | 52       |\n",
      "|    iterations         | 6        |\n",
      "|    time_elapsed       | 1001     |\n",
      "|    total_timesteps    | 52920    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.228   |\n",
      "|    explained_variance | 0.245    |\n",
      "|    learning_rate      | 0.0006   |\n",
      "|    n_updates          | 5        |\n",
      "|    policy_loss        | -0.01    |\n",
      "|    value_loss         | 1.48e+03 |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 588      |\n",
      "|    ep_rew_mean        | 499      |\n",
      "| time/                 |          |\n",
      "|    fps                | 52       |\n",
      "|    iterations         | 7        |\n",
      "|    time_elapsed       | 1171     |\n",
      "|    total_timesteps    | 61740    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.228   |\n",
      "|    explained_variance | 0.183    |\n",
      "|    learning_rate      | 0.0006   |\n",
      "|    n_updates          | 6        |\n",
      "|    policy_loss        | -0.0222  |\n",
      "|    value_loss         | 2.08e+03 |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 593      |\n",
      "|    ep_rew_mean        | 488      |\n",
      "| time/                 |          |\n",
      "|    fps                | 52       |\n",
      "|    iterations         | 8        |\n",
      "|    time_elapsed       | 1341     |\n",
      "|    total_timesteps    | 70560    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.254   |\n",
      "|    explained_variance | 0.209    |\n",
      "|    learning_rate      | 0.0006   |\n",
      "|    n_updates          | 7        |\n",
      "|    policy_loss        | 0.0029   |\n",
      "|    value_loss         | 1.15e+03 |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 597      |\n",
      "|    ep_rew_mean        | 489      |\n",
      "| time/                 |          |\n",
      "|    fps                | 51       |\n",
      "|    iterations         | 9        |\n",
      "|    time_elapsed       | 1526     |\n",
      "|    total_timesteps    | 79380    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.237   |\n",
      "|    explained_variance | 0.315    |\n",
      "|    learning_rate      | 0.0006   |\n",
      "|    n_updates          | 8        |\n",
      "|    policy_loss        | -0.00889 |\n",
      "|    value_loss         | 1.38e+03 |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 608       |\n",
      "|    ep_rew_mean        | 482       |\n",
      "| time/                 |           |\n",
      "|    fps                | 52        |\n",
      "|    iterations         | 10        |\n",
      "|    time_elapsed       | 1694      |\n",
      "|    total_timesteps    | 88200     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.242    |\n",
      "|    explained_variance | 0.328     |\n",
      "|    learning_rate      | 0.0006    |\n",
      "|    n_updates          | 9         |\n",
      "|    policy_loss        | -0.000463 |\n",
      "|    value_loss         | 1.08e+03  |\n",
      "-------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 616      |\n",
      "|    ep_rew_mean        | 485      |\n",
      "| time/                 |          |\n",
      "|    fps                | 52       |\n",
      "|    iterations         | 11       |\n",
      "|    time_elapsed       | 1861     |\n",
      "|    total_timesteps    | 97020    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.249   |\n",
      "|    explained_variance | 0.337    |\n",
      "|    learning_rate      | 0.0006   |\n",
      "|    n_updates          | 10       |\n",
      "|    policy_loss        | -0.00171 |\n",
      "|    value_loss         | 1.96e+03 |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 611       |\n",
      "|    ep_rew_mean        | 492       |\n",
      "| time/                 |           |\n",
      "|    fps                | 52        |\n",
      "|    iterations         | 12        |\n",
      "|    time_elapsed       | 2027      |\n",
      "|    total_timesteps    | 105840    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.237    |\n",
      "|    explained_variance | 0.38      |\n",
      "|    learning_rate      | 0.0006    |\n",
      "|    n_updates          | 11        |\n",
      "|    policy_loss        | -0.000392 |\n",
      "|    value_loss         | 1.08e+03  |\n",
      "-------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 616      |\n",
      "|    ep_rew_mean        | 491      |\n",
      "| time/                 |          |\n",
      "|    fps                | 52       |\n",
      "|    iterations         | 13       |\n",
      "|    time_elapsed       | 2190     |\n",
      "|    total_timesteps    | 114660   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.238   |\n",
      "|    explained_variance | 0.321    |\n",
      "|    learning_rate      | 0.0006   |\n",
      "|    n_updates          | 12       |\n",
      "|    policy_loss        | -0.00715 |\n",
      "|    value_loss         | 1.99e+03 |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 606      |\n",
      "|    ep_rew_mean        | 501      |\n",
      "| time/                 |          |\n",
      "|    fps                | 52       |\n",
      "|    iterations         | 14       |\n",
      "|    time_elapsed       | 2351     |\n",
      "|    total_timesteps    | 123480   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.231   |\n",
      "|    explained_variance | 0.238    |\n",
      "|    learning_rate      | 0.0006   |\n",
      "|    n_updates          | 13       |\n",
      "|    policy_loss        | 0.0148   |\n",
      "|    value_loss         | 2.31e+03 |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 619      |\n",
      "|    ep_rew_mean        | 490      |\n",
      "| time/                 |          |\n",
      "|    fps                | 52       |\n",
      "|    iterations         | 15       |\n",
      "|    time_elapsed       | 2501     |\n",
      "|    total_timesteps    | 132300   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.234   |\n",
      "|    explained_variance | 0.37     |\n",
      "|    learning_rate      | 0.0006   |\n",
      "|    n_updates          | 14       |\n",
      "|    policy_loss        | 0.00869  |\n",
      "|    value_loss         | 1.73e+03 |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 625      |\n",
      "|    ep_rew_mean        | 516      |\n",
      "| time/                 |          |\n",
      "|    fps                | 53       |\n",
      "|    iterations         | 16       |\n",
      "|    time_elapsed       | 2632     |\n",
      "|    total_timesteps    | 141120   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.232   |\n",
      "|    explained_variance | 0.392    |\n",
      "|    learning_rate      | 0.0006   |\n",
      "|    n_updates          | 15       |\n",
      "|    policy_loss        | 0.0142   |\n",
      "|    value_loss         | 1.85e+03 |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 623      |\n",
      "|    ep_rew_mean        | 516      |\n",
      "| time/                 |          |\n",
      "|    fps                | 54       |\n",
      "|    iterations         | 17       |\n",
      "|    time_elapsed       | 2746     |\n",
      "|    total_timesteps    | 149940   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.236   |\n",
      "|    explained_variance | 0.395    |\n",
      "|    learning_rate      | 0.0006   |\n",
      "|    n_updates          | 16       |\n",
      "|    policy_loss        | -0.0142  |\n",
      "|    value_loss         | 1.4e+03  |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 618      |\n",
      "|    ep_rew_mean        | 526      |\n",
      "| time/                 |          |\n",
      "|    fps                | 55       |\n",
      "|    iterations         | 18       |\n",
      "|    time_elapsed       | 2859     |\n",
      "|    total_timesteps    | 158760   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.231   |\n",
      "|    explained_variance | 0.364    |\n",
      "|    learning_rate      | 0.0006   |\n",
      "|    n_updates          | 17       |\n",
      "|    policy_loss        | 0.0121   |\n",
      "|    value_loss         | 976      |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 608      |\n",
      "|    ep_rew_mean        | 499      |\n",
      "| time/                 |          |\n",
      "|    fps                | 55       |\n",
      "|    iterations         | 19       |\n",
      "|    time_elapsed       | 3014     |\n",
      "|    total_timesteps    | 167580   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.224   |\n",
      "|    explained_variance | 0.0734   |\n",
      "|    learning_rate      | 0.0006   |\n",
      "|    n_updates          | 18       |\n",
      "|    policy_loss        | -0.0057  |\n",
      "|    value_loss         | 1.88e+03 |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 598      |\n",
      "|    ep_rew_mean        | 469      |\n",
      "| time/                 |          |\n",
      "|    fps                | 55       |\n",
      "|    iterations         | 20       |\n",
      "|    time_elapsed       | 3182     |\n",
      "|    total_timesteps    | 176400   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.242   |\n",
      "|    explained_variance | 0.412    |\n",
      "|    learning_rate      | 0.0006   |\n",
      "|    n_updates          | 19       |\n",
      "|    policy_loss        | 0.00806  |\n",
      "|    value_loss         | 1.11e+03 |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 604      |\n",
      "|    ep_rew_mean        | 468      |\n",
      "| time/                 |          |\n",
      "|    fps                | 54       |\n",
      "|    iterations         | 21       |\n",
      "|    time_elapsed       | 3370     |\n",
      "|    total_timesteps    | 185220   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.241   |\n",
      "|    explained_variance | 0.261    |\n",
      "|    learning_rate      | 0.0006   |\n",
      "|    n_updates          | 20       |\n",
      "|    policy_loss        | 0.00308  |\n",
      "|    value_loss         | 1.68e+03 |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 619      |\n",
      "|    ep_rew_mean        | 476      |\n",
      "| time/                 |          |\n",
      "|    fps                | 54       |\n",
      "|    iterations         | 22       |\n",
      "|    time_elapsed       | 3551     |\n",
      "|    total_timesteps    | 194040   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.252   |\n",
      "|    explained_variance | 0.512    |\n",
      "|    learning_rate      | 0.0006   |\n",
      "|    n_updates          | 21       |\n",
      "|    policy_loss        | -0.00594 |\n",
      "|    value_loss         | 1.16e+03 |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 596      |\n",
      "|    ep_rew_mean        | 490      |\n",
      "| time/                 |          |\n",
      "|    fps                | 54       |\n",
      "|    iterations         | 23       |\n",
      "|    time_elapsed       | 3722     |\n",
      "|    total_timesteps    | 202860   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.244   |\n",
      "|    explained_variance | 0.3      |\n",
      "|    learning_rate      | 0.0006   |\n",
      "|    n_updates          | 22       |\n",
      "|    policy_loss        | -0.00448 |\n",
      "|    value_loss         | 2.12e+03 |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 591      |\n",
      "|    ep_rew_mean        | 482      |\n",
      "| time/                 |          |\n",
      "|    fps                | 54       |\n",
      "|    iterations         | 24       |\n",
      "|    time_elapsed       | 3874     |\n",
      "|    total_timesteps    | 211680   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.244   |\n",
      "|    explained_variance | 0.241    |\n",
      "|    learning_rate      | 0.0006   |\n",
      "|    n_updates          | 23       |\n",
      "|    policy_loss        | -0.00844 |\n",
      "|    value_loss         | 2.48e+03 |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 596      |\n",
      "|    ep_rew_mean        | 476      |\n",
      "| time/                 |          |\n",
      "|    fps                | 55       |\n",
      "|    iterations         | 25       |\n",
      "|    time_elapsed       | 3979     |\n",
      "|    total_timesteps    | 220500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.242   |\n",
      "|    explained_variance | 0.337    |\n",
      "|    learning_rate      | 0.0006   |\n",
      "|    n_updates          | 24       |\n",
      "|    policy_loss        | -0.0162  |\n",
      "|    value_loss         | 1.72e+03 |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 591      |\n",
      "|    ep_rew_mean        | 460      |\n",
      "| time/                 |          |\n",
      "|    fps                | 56       |\n",
      "|    iterations         | 26       |\n",
      "|    time_elapsed       | 4087     |\n",
      "|    total_timesteps    | 229320   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.226   |\n",
      "|    explained_variance | 0.283    |\n",
      "|    learning_rate      | 0.0006   |\n",
      "|    n_updates          | 25       |\n",
      "|    policy_loss        | -0.00197 |\n",
      "|    value_loss         | 1.73e+03 |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 602      |\n",
      "|    ep_rew_mean        | 463      |\n",
      "| time/                 |          |\n",
      "|    fps                | 56       |\n",
      "|    iterations         | 27       |\n",
      "|    time_elapsed       | 4239     |\n",
      "|    total_timesteps    | 238140   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.235   |\n",
      "|    explained_variance | 0.384    |\n",
      "|    learning_rate      | 0.0006   |\n",
      "|    n_updates          | 26       |\n",
      "|    policy_loss        | 0.0014   |\n",
      "|    value_loss         | 1.09e+03 |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 612      |\n",
      "|    ep_rew_mean        | 473      |\n",
      "| time/                 |          |\n",
      "|    fps                | 55       |\n",
      "|    iterations         | 28       |\n",
      "|    time_elapsed       | 4435     |\n",
      "|    total_timesteps    | 246960   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.234   |\n",
      "|    explained_variance | 0.277    |\n",
      "|    learning_rate      | 0.0006   |\n",
      "|    n_updates          | 27       |\n",
      "|    policy_loss        | -0.0048  |\n",
      "|    value_loss         | 1.41e+03 |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 606      |\n",
      "|    ep_rew_mean        | 461      |\n",
      "| time/                 |          |\n",
      "|    fps                | 55       |\n",
      "|    iterations         | 29       |\n",
      "|    time_elapsed       | 4644     |\n",
      "|    total_timesteps    | 255780   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.241   |\n",
      "|    explained_variance | 0.301    |\n",
      "|    learning_rate      | 0.0006   |\n",
      "|    n_updates          | 28       |\n",
      "|    policy_loss        | 0.00591  |\n",
      "|    value_loss         | 1.65e+03 |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 627      |\n",
      "|    ep_rew_mean        | 484      |\n",
      "| time/                 |          |\n",
      "|    fps                | 54       |\n",
      "|    iterations         | 30       |\n",
      "|    time_elapsed       | 4837     |\n",
      "|    total_timesteps    | 264600   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.263   |\n",
      "|    explained_variance | 0.26     |\n",
      "|    learning_rate      | 0.0006   |\n",
      "|    n_updates          | 29       |\n",
      "|    policy_loss        | 0.0166   |\n",
      "|    value_loss         | 1.32e+03 |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 631      |\n",
      "|    ep_rew_mean        | 468      |\n",
      "| time/                 |          |\n",
      "|    fps                | 54       |\n",
      "|    iterations         | 31       |\n",
      "|    time_elapsed       | 4973     |\n",
      "|    total_timesteps    | 273420   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.248   |\n",
      "|    explained_variance | 0.288    |\n",
      "|    learning_rate      | 0.0006   |\n",
      "|    n_updates          | 30       |\n",
      "|    policy_loss        | -0.00271 |\n",
      "|    value_loss         | 1.8e+03  |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 628       |\n",
      "|    ep_rew_mean        | 478       |\n",
      "| time/                 |           |\n",
      "|    fps                | 54        |\n",
      "|    iterations         | 32        |\n",
      "|    time_elapsed       | 5148      |\n",
      "|    total_timesteps    | 282240    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.257    |\n",
      "|    explained_variance | 0.458     |\n",
      "|    learning_rate      | 0.0006    |\n",
      "|    n_updates          | 31        |\n",
      "|    policy_loss        | -0.000971 |\n",
      "|    value_loss         | 1.66e+03  |\n",
      "-------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 620      |\n",
      "|    ep_rew_mean        | 480      |\n",
      "| time/                 |          |\n",
      "|    fps                | 54       |\n",
      "|    iterations         | 33       |\n",
      "|    time_elapsed       | 5372     |\n",
      "|    total_timesteps    | 291060   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.238   |\n",
      "|    explained_variance | 0.318    |\n",
      "|    learning_rate      | 0.0006   |\n",
      "|    n_updates          | 32       |\n",
      "|    policy_loss        | 0.0149   |\n",
      "|    value_loss         | 1.63e+03 |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 642      |\n",
      "|    ep_rew_mean        | 529      |\n",
      "| time/                 |          |\n",
      "|    fps                | 53       |\n",
      "|    iterations         | 34       |\n",
      "|    time_elapsed       | 5583     |\n",
      "|    total_timesteps    | 299880   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.253   |\n",
      "|    explained_variance | 0.2      |\n",
      "|    learning_rate      | 0.0006   |\n",
      "|    n_updates          | 33       |\n",
      "|    policy_loss        | 0.0188   |\n",
      "|    value_loss         | 2.5e+03  |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 639      |\n",
      "|    ep_rew_mean        | 542      |\n",
      "| time/                 |          |\n",
      "|    fps                | 53       |\n",
      "|    iterations         | 35       |\n",
      "|    time_elapsed       | 5730     |\n",
      "|    total_timesteps    | 308700   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.241   |\n",
      "|    explained_variance | 0.188    |\n",
      "|    learning_rate      | 0.0006   |\n",
      "|    n_updates          | 34       |\n",
      "|    policy_loss        | 0.0218   |\n",
      "|    value_loss         | 1.91e+03 |\n",
      "------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from stable_baselines3 import A2C\n",
    "\n",
    "steps = [(\n",
    "    # 1024*8,\n",
    "    20*n_envs,\n",
    "    300_000,\n",
    ")]\n",
    "for n_steps, total_timesteps in steps:\n",
    "    # model = PPO(\n",
    "    #     \"MlpPolicy\", \n",
    "    #     vec_env, \n",
    "    #     verbose=1, \n",
    "    #     policy_kwargs = dict(net_arch=net_arch, activation_fn=activation_fn,),\n",
    "    #     device='cpu',\n",
    "    #     learning_rate=0.0001,\n",
    "    #     batch_size=128,\n",
    "    #     n_epochs=100,\n",
    "    #     n_steps=n_steps,\n",
    "    #     tensorboard_log=\"./outputs/\",\n",
    "    #     # ent_coef=0.001,\n",
    "    #     clip_range=0.2,\n",
    "    # )\n",
    "    model = A2C(\n",
    "        \"MlpPolicy\", \n",
    "        vec_env, \n",
    "        verbose=1, \n",
    "        policy_kwargs = dict(net_arch=net_arch, activation_fn=activation_fn,),\n",
    "        device='cpu',\n",
    "        learning_rate=0.0006,\n",
    "        n_steps=n_steps,\n",
    "        tensorboard_log=\"./outputs/\",\n",
    "        use_rms_prop=False,\n",
    "        normalize_advantage=True,\n",
    "    )\n",
    "    print('DOING', n_steps, total_timesteps)\n",
    "    model.policy.load_state_dict(unified_policy.shared.state_dict())\n",
    "    model.learn(total_timesteps=total_timesteps, progress_bar=True, log_interval=1)\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'final_a2c_420'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save(f'final_a2c_{n_steps}')\n",
    "f'final_a2c_{n_steps}'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bbrl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
